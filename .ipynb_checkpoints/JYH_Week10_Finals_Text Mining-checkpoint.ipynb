{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트마이닝으로 정서분류\n",
    "#### 정서란? \n",
    "- 사람의 마음에 일어나는 여러 가지 감정 또는 감정을 불러일으키는 기분이나 분위기\n",
    "\n",
    "#### 한국문화, 한류 등의 주제로 긍정 or 부정 등의 정서 분류 하기\n",
    "\n",
    "- 연구가설 : 한국 사람들의 한류에 대한 생각이 긍정적 일 것이다.\n",
    "- 연구방법 : 네이버 블로그, 다음 블로그 내용 크롤링 해서 긍정, 부정 분류\n",
    "- 결론 : 한국 사람들의 한류에 대한 생각이 89% 정도가 긍정적이다. (300개 중 267개)\n",
    "\n",
    "#### 분류하기\n",
    "- NaiveBayesian으로 분류하기\n",
    "- SVM(LIBSVM)으로 분류하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. 데이터 수집 : RSS 이용\n",
    "- 네이버 블로그, 다음 블로그 중심으로\n",
    "- 키워드 : 한류, 반한류, 혐한류\n",
    "- 키워드로 검색후 나온 페이지의 rss 주소를 urls_rss.txt 에 저장 (210개의 rss 주소 수집)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python2.7\n",
    "# -*- coding: utf-8 -*-\n",
    "import feedparser\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "def spider():\n",
    "    \n",
    "    with open('python_week10/urls_rss.txt') as input_file:\n",
    "        \n",
    "        count = 0\n",
    "        keyword = '한류'\n",
    "        \n",
    "        for i, line in enumerate(input_file):\n",
    "\n",
    "            url_info = line.split('\\t')\n",
    "            url = url_info[0]\n",
    "            rss = feedparser.parse(url)\n",
    "\n",
    "            for post in rss.entries:\n",
    "                final_text = \"\"\n",
    "                final_text = post.description\n",
    "                if(keyword in final_text):\n",
    "                    count = count + 1\n",
    "                    file_name = (\"python_week10/doc_rss/%s.txt\" % count)\n",
    "                    out = open(file_name, 'w')\n",
    "                    #print(final_text)\n",
    "                    print >> out, final_text\n",
    "                    out.close()\n",
    "                    \n",
    "spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. 데이터 분류 : 수작업\n",
    "- 수집된 300개의 데이터를 내용을 보고 긍정, 부정으로 분류\n",
    "- 긍정은 doc_rss>p 폴더, 부정은 doc_rss>n 폴더"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. 자연어처리 (NLP)\n",
    "- NLP는 대표적인 한글 파이썬 NLP인 konlpy 이용\n",
    "- 분류된 텍스트를 NLP를 이용해 명사, 동사, 형용사 위주로 분리\n",
    "- 텍스트 분리후 doc_nlp.txt 에 한꺼번에 저장 (긍정은 1, 부정은 0 으로 태깅. 267개:33개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python2.7\n",
    "# -*- coding: utf-8 -*-\n",
    "import konlpy\n",
    "import nltk\n",
    "import os.path\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "def nlp():\n",
    "    \n",
    "    # Define a chunk grammar, or chunking rules, then chunk\n",
    "    grammar = \"\"\"\n",
    "    NNG: {<NNG.*>*}   # Noun phrase\n",
    "    NNP: {<NNP.*>*}   # Noun phrase\n",
    "    NP: {<NP.*>*}     # Noun phrase\n",
    "    VV: {<VV.*>*}     # Verb phrase\n",
    "    VA: {<VA.*>*}     # Adjective phrase\n",
    "    \"\"\"\n",
    "\n",
    "    # 초기화\n",
    "    file_name = \"python_week10/doc_rss/doc_nlp.txt\"\n",
    "    open(file_name, 'w').close()\n",
    "    \n",
    "    for no in range(1, 301):\n",
    "    \n",
    "        txt_file = 'python_week10/doc_rss/p/%s.txt' % no\n",
    "        if(os.path.isfile(txt_file)):\n",
    "            pn_type = '1'\n",
    "        else:\n",
    "            txt_file = 'python_week10/doc_rss/n/%s.txt' % no\n",
    "            pn_type = '0'\n",
    "\n",
    "        with open(txt_file) as input_file:\n",
    "\n",
    "            sentence = \"\"\n",
    "            for i, line in enumerate(input_file):\n",
    "                sentence = sentence + ' ' + line\n",
    "\n",
    "            words = konlpy.tag.Kkma().pos(sentence)\n",
    "\n",
    "            #print(words)\n",
    "            parser = nltk.RegexpParser(grammar)\n",
    "            chunks = parser.parse(words)\n",
    "\n",
    "            #word_arr = set([])\n",
    "            word_arr = pn_type + \"\\t\"\n",
    "            for subtree in chunks.subtrees():\n",
    "                if subtree.label()=='NNG' or subtree.label()=='NNP' or subtree.label()=='NP' or subtree.label()=='VV' or subtree.label()=='VA':\n",
    "                    for sub in subtree:\n",
    "                        try:\n",
    "                            #word_arr = word_arr | set([sub.__getitem__(0)])\n",
    "                            word_arr = word_arr + sub.__getitem__(0) + ' '\n",
    "                        except UnicodeEncodeError:\n",
    "                            pass\n",
    "\n",
    "            out = open(file_name, 'a')\n",
    "            #print(unicode(word_arr))\n",
    "            print >> out, word_arr\n",
    "            out.close()\n",
    "        \n",
    "nlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. 베이지안\n",
    "#### 4.1 단어 벡터 만들기\n",
    "- train set\n",
    "- test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python2.7\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "\n",
    "def loadDataSet():\n",
    "    postingList = []\n",
    "    classVec = []\n",
    "    \n",
    "    with open('python_week10/doc_rss/doc_nlp.txt') as input_file:\n",
    "        \n",
    "        for i, line in enumerate(input_file):\n",
    "\n",
    "            info = line.split('\\t')\n",
    "            pos_neg = info[0]\n",
    "            text = info[1]\n",
    "            \n",
    "            postingList.append(text.split())\n",
    "            classVec.append(int(pos_neg))\n",
    "            \n",
    "    return postingList,classVec\n",
    "                 \n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print \"the word: %s is not in my Vocabulary!\" % word\n",
    "    return returnVec\n",
    "\n",
    "\n",
    "\n",
    "# listOPosts, listClasses = loadDataSet()\n",
    "# myVocabList = createVocabList(listOPosts)\n",
    "# print unicode(myVocabList[0])\n",
    "# #print setOfWords2Vec(myVocabList, listOPosts[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 훈련 : 단어 벡터로 확률 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python2.7\n",
    "# -*- coding: utf-8 -*-\n",
    "from numpy import *\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) #+ log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) #+ log(1.0 - pClass1)\n",
    "    \n",
    "    print('p1: %s, p0: %s' % (p1, p0))\n",
    "    \n",
    "    if p1 > p0:\n",
    "        return u'긍정'\n",
    "    else: \n",
    "        return u'부정'\n",
    "    \n",
    "    \n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "        \n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = zeros(numWords); p1Num = zeros(numWords)      #change to ones() \n",
    "    p0Denom = 0.0; p1Denom = 0.0                        #change to 2.0\n",
    "#     p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() \n",
    "#     p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "#     p1Vect = log(p1Num/p1Denom)          #change to log()\n",
    "#     p0Vect = log(p0Num/p0Denom)          #change to log()\n",
    "    p1Vect = p1Num/p1Denom          #change to log()\n",
    "    p0Vect = p0Num/p0Denom          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. 최종 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python2.7\n",
    "# -*- coding: utf-8 -*-\n",
    "import feedparser\n",
    "import konlpy\n",
    "import nltk\n",
    "from numpy import *\n",
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf-8')\n",
    "\n",
    "\n",
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    \n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "        \n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "        \n",
    "#     print pAb\n",
    "#     print p0V\n",
    "#     print p1V    \n",
    "    \n",
    "    \n",
    "    # Define a chunk grammar, or chunking rules, then chunk\n",
    "    grammar = \"\"\"\n",
    "    NNG: {<NNG.*>*}   # Noun phrase\n",
    "    NNP: {<NNP.*>*}   # Noun phrase\n",
    "    NP: {<NP.*>*}     # Noun phrase\n",
    "    VV: {<VV.*>*}     # Verb phrase\n",
    "    VA: {<VA.*>*}     # Adjective phrase\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # 테스트할 문장\n",
    "    \n",
    "    ## 긍정\n",
    "    rss = feedparser.parse('http://blog.rss.naver.com/myth2030.xml')\n",
    "    ## 부정\n",
    "#     rss = feedparser.parse('http://blog.rss.naver.com/golf798kang5.xml')\n",
    "\n",
    "    for post in rss.entries:\n",
    "        final_text = \"\"\n",
    "        final_text = post.description\n",
    "        \n",
    "        if('한류' in final_text):\n",
    "            sentence = unicode(final_text)\n",
    "#             sentence = u\"방 중국 어제 보 중국인 무감각 표정 맛 멋 없 언행 나 이번 방 미사일 초토화되 중국 미소 지켜보 태양 후예 드라마 황 치열 가수 어떠 사람 말하 한류 중국 동남아 공산권 회교권 강타 이유 감성 차 하 맞 말 보 공산주의 유물론 젖 영혼 물질 팔아먹 중국 강압적 교리 묶이 본성 허리띠 묶이 회교권 향하 날아가 한류 미사일 핵폭탄 퍼지 수년 전 하 상상 현상 작가 나 뛰어나 감성 배우 감성 연기 중국 녹 회교권 다운 이 세기 감성 시대 사실 웅변 거대 경제력 엄청나 외환 보유고 거기 다그 각종 첨단 무기 우주 개발 기술 전 세계 뒤덮 같 중국 한국 드라마 하나 한류 가수 의하 대륙 들끓 정부 간섭 하 정도 어디 둘 드라마 흉내 내서 치킨 맥주 잔 마시 장면 연출 위하 한국 새카맣 몰려오 나 그만큼 감성 후지 약하 그 중국 최대 약점 아킬레스건 유구 역사 속 문화 예술 유전 인자 가지 그 공산주의 유물론 치 정신문화 후진국 되 거기 비하 복음 서양 선교사 전하 받 하 복음 인하 나라 밝 되 발전 발전 하 되 주 초가 되 지금 감성 문화 발달 저변 복음 전파 교회 공로 가 지대 말하 있 계몽 시대 지나 음 표현 노래 문화 교회 이끌 하 과언 열매 사회 저변 깔리 국가 전체 감성 흐름 주도 말하 있 정 보도 기술 세계 평준화 되 앞서 뒤서 하 중국 재벌 영화 촬영 세트 미국 할리우드 크 짓 떠들 중국 그렇 하 그러 영혼 노래 감성 흐르 표정 언어 없 연출자 배우 우리 같 드라마 영화 기대 있 그 숙제 되 가수 드라마 그 가슴 울리 웃기 하 나 이번 방 핵 미 사실 같 한류 폭탄 맞 중국 보 그 쉽 생각 하 보 세계 최대 경제 대국 인구 가지 중국 우리 앞 마당 있 장터 느낌 지우 없 \"\n",
    "            words = konlpy.tag.Kkma().pos(sentence)\n",
    "\n",
    "            #print(words)\n",
    "            parser = nltk.RegexpParser(grammar)\n",
    "            chunks = parser.parse(words)\n",
    "\n",
    "            #word_arr = set([])\n",
    "            word_arr = \"\"\n",
    "            for subtree in chunks.subtrees():\n",
    "                if subtree.label()=='NNG' or subtree.label()=='NNP' or subtree.label()=='NP' or subtree.label()=='VV' or subtree.label()=='VA':\n",
    "                    for sub in subtree:\n",
    "                        try:\n",
    "                            word_arr = word_arr + sub.__getitem__(0) + ' '\n",
    "                        except UnicodeEncodeError:\n",
    "                            pass\n",
    "\n",
    "            testEntry = word_arr.split()\n",
    "            thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "            print sentence,'\\n classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "    \n",
    "    \n",
    "testingNB()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. SVM (LIBSVM)\n",
    "#### 5.1 LIBSVM 설치\n",
    "- http://www.lfd.uci.edu/~gohlke/pythonlibs/#libsvm\n",
    "- libsvm-3.21-cp27-none-win_amd64.whl 파일 다운로드 후 pip install로 설치\n",
    "\n",
    "#### 5.2 LIBSVM 이용 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " (100.0, 0.0, 1.0),\n",
       " [[1.3639360789019588],\n",
       "  [1.0779763709060521],\n",
       "  [1.0240538325416133],\n",
       "  [1.0343378425655478],\n",
       "  [0.9999175406072153],\n",
       "  [0.999570531071099],\n",
       "  [1.0664705611681515],\n",
       "  [0.9996103968196769],\n",
       "  [1.0536701162181028],\n",
       "  [1.2112233117110098],\n",
       "  [1.0001758231759852],\n",
       "  [0.9998135753963876],\n",
       "  [-1.0000919842249378],\n",
       "  [1.1072078588637622],\n",
       "  [1.0560505650778846],\n",
       "  [1.0002390344252639],\n",
       "  [-0.9999441926993792],\n",
       "  [0.9999360571004128],\n",
       "  [-1.0000047670050058],\n",
       "  [1.0001777617552872],\n",
       "  [1.0002926388095572],\n",
       "  [1.0003421648300792],\n",
       "  [-1.0002093551227236],\n",
       "  [1.02685313855392],\n",
       "  [1.276306770580796],\n",
       "  [0.9999434547587918],\n",
       "  [-1.0001488749661651],\n",
       "  [1.0001112039111066],\n",
       "  [1.000004054243365],\n",
       "  [1.0699793569915867],\n",
       "  [0.9999216018888557],\n",
       "  [1.0469938736121644],\n",
       "  [-1.0001118317814313],\n",
       "  [-1.0000028063274402],\n",
       "  [1.0002444818544496],\n",
       "  [1.0117533100094744],\n",
       "  [1.0004454024552607],\n",
       "  [0.9997506091442785],\n",
       "  [1.0169438628059684],\n",
       "  [1.1153634435477335],\n",
       "  [1.0003248448146504],\n",
       "  [0.9995365025868415],\n",
       "  [0.9997851673281435],\n",
       "  [1.0478228145178123],\n",
       "  [0.9996753769729556],\n",
       "  [1.164515953058771],\n",
       "  [1.3451594165021676],\n",
       "  [1.1275685912688374],\n",
       "  [1.0845368549406975],\n",
       "  [0.9995936036674495],\n",
       "  [0.9996079912042464],\n",
       "  [1.0000094865510185],\n",
       "  [1.0002027942486016],\n",
       "  [1.0032253125741182],\n",
       "  [1.0004089377324639],\n",
       "  [-1.0000299573810034],\n",
       "  [-0.9998572076628279],\n",
       "  [1.0002890720520203],\n",
       "  [1.0002227440681621],\n",
       "  [1.0003425308311398],\n",
       "  [1.0001608265046933],\n",
       "  [-0.9997185973305164],\n",
       "  [-0.9996185439704671],\n",
       "  [-0.9996176757754077],\n",
       "  [-1.000069445906158],\n",
       "  [-0.9997971636198285],\n",
       "  [-1.0003094850089425],\n",
       "  [1.0000429317890276],\n",
       "  [1.022677346836452],\n",
       "  [0.9995846766192265],\n",
       "  [1.0001138382177714],\n",
       "  [1.0002036177001599],\n",
       "  [0.9996989430164586],\n",
       "  [0.9995596820349333],\n",
       "  [0.9998545540021008],\n",
       "  [1.000211546979583],\n",
       "  [-1.0001575934697358],\n",
       "  [-1.000064261921282],\n",
       "  [1.0522441468035242],\n",
       "  [1.0571640987289908],\n",
       "  [0.999776703787169],\n",
       "  [1.0218348340212886],\n",
       "  [0.9997285959544158],\n",
       "  [0.9997944045714003],\n",
       "  [0.9997030401021398],\n",
       "  [0.9996430980746884],\n",
       "  [1.000070729385431],\n",
       "  [1.0002753668123683],\n",
       "  [1.0003343220280534],\n",
       "  [1.1699622209573541],\n",
       "  [0.999967229196012],\n",
       "  [0.9997532336550924],\n",
       "  [-1.0000254154202164],\n",
       "  [-0.9998912596114448],\n",
       "  [-1.0000601126645239],\n",
       "  [1.0004252064617827],\n",
       "  [1.0002002051357097],\n",
       "  [1.0002737661080165],\n",
       "  [1.00030547646459],\n",
       "  [-0.9995600427125463],\n",
       "  [0.9999915904863641],\n",
       "  [1.000110986883016],\n",
       "  [1.0001162511373813],\n",
       "  [-0.9999974202671517],\n",
       "  [1.0378105060182032],\n",
       "  [1.1660837313166064],\n",
       "  [1.0003400759508843],\n",
       "  [1.275056095943166],\n",
       "  [1.126111025226299],\n",
       "  [1.5452678839668321],\n",
       "  [1.1658678164495007],\n",
       "  [1.2034831804245294],\n",
       "  [1.1589211678066504],\n",
       "  [0.9999602522140383],\n",
       "  [1.0675901732066224],\n",
       "  [0.9996248842647173],\n",
       "  [-1.0001826194711785],\n",
       "  [1.1772921061013593],\n",
       "  [1.000191254837274],\n",
       "  [1.0001781357200166],\n",
       "  [1.0529578342450558],\n",
       "  [1.0004281914400033],\n",
       "  [1.278209002542976],\n",
       "  [1.1592139490818893],\n",
       "  [1.2219974308485455],\n",
       "  [1.2235406533237234],\n",
       "  [1.2022639082800297],\n",
       "  [1.2260920983886079],\n",
       "  [1.0935541971380531],\n",
       "  [0.9996090571142269],\n",
       "  [1.2573628597347977],\n",
       "  [0.9999547516785496],\n",
       "  [1.1705551929959068],\n",
       "  [1.0833274484585425],\n",
       "  [1.2302898835516443],\n",
       "  [0.9994942963114684],\n",
       "  [1.1776414943954467],\n",
       "  [1.000022742054596],\n",
       "  [1.1240175617936847],\n",
       "  [1.000077583909512],\n",
       "  [1.0600904573075887],\n",
       "  [0.9995489756016532],\n",
       "  [0.9998693665818672],\n",
       "  [1.0004122134800986],\n",
       "  [1.000116513054682],\n",
       "  [1.2674389113441058],\n",
       "  [1.2819994059009403],\n",
       "  [1.1677969150756486],\n",
       "  [1.0000400677483787],\n",
       "  [1.0003567747212079],\n",
       "  [1.0172075834352419],\n",
       "  [0.9999581489766919],\n",
       "  [1.0001121150698615],\n",
       "  [1.4734423264676773],\n",
       "  [0.9996905636322029],\n",
       "  [-1.0002684859503095],\n",
       "  [1.0001569212252839],\n",
       "  [1.000012753297703],\n",
       "  [1.0001500380658248],\n",
       "  [1.2059319205125103],\n",
       "  [1.0004789754175367],\n",
       "  [0.999568639030992],\n",
       "  [1.000105286830936],\n",
       "  [1.0003709520028994],\n",
       "  [1.1269713765374747],\n",
       "  [1.1794727169944745],\n",
       "  [0.9997305493857516],\n",
       "  [1.000056339389836],\n",
       "  [1.000441979194049],\n",
       "  [0.9999606577262098],\n",
       "  [1.0003334124604844],\n",
       "  [1.0002722814678486],\n",
       "  [1.000374865015858],\n",
       "  [0.9999103111439244],\n",
       "  [0.9999224080265268],\n",
       "  [0.9997863372039676],\n",
       "  [1.0001915053507155],\n",
       "  [-1.0001943590603757],\n",
       "  [1.0000469043519284],\n",
       "  [0.9995775230002086],\n",
       "  [1.31979852023201],\n",
       "  [1.0434505681030644],\n",
       "  [1.1443598648877722],\n",
       "  [1.0004352416876716],\n",
       "  [1.0001393125626614],\n",
       "  [1.004530791277948],\n",
       "  [0.9995224653911441],\n",
       "  [1.2728788919349296],\n",
       "  [0.9999293806919577],\n",
       "  [1.1912834778291077],\n",
       "  [0.9998657449207909],\n",
       "  [0.9998300242560807],\n",
       "  [1.1329584815900782],\n",
       "  [1.000306707203329],\n",
       "  [0.9997077235206329],\n",
       "  [0.999952809044257],\n",
       "  [0.9997227473448367],\n",
       "  [1.1056032805732043],\n",
       "  [0.9995205647876015],\n",
       "  [1.0000044875586935],\n",
       "  [0.9998967846292777],\n",
       "  [0.9997784523756945],\n",
       "  [1.0001160307190546],\n",
       "  [0.9996601832906489],\n",
       "  [1.000300243972362],\n",
       "  [1.1207838089465296],\n",
       "  [1.054335901747995],\n",
       "  [0.9995199749261456],\n",
       "  [1.0000731260399478],\n",
       "  [0.9996235907736579],\n",
       "  [0.9999498866146534],\n",
       "  [1.2129281594600583],\n",
       "  [1.3020406969179459],\n",
       "  [1.1129261008354303],\n",
       "  [0.9999614735821275],\n",
       "  [1.0000119063863904],\n",
       "  [1.1156858386711814],\n",
       "  [1.0001974651419447],\n",
       "  [1.1981045379262816],\n",
       "  [1.2533507641720063],\n",
       "  [1.0001618026568897],\n",
       "  [1.0002275478386098],\n",
       "  [-0.9996732693371008],\n",
       "  [1.0002393198793698],\n",
       "  [0.9999514579359824],\n",
       "  [0.999976267375061],\n",
       "  [1.0000951335006338],\n",
       "  [1.176783887240189],\n",
       "  [0.999711119506447],\n",
       "  [1.0002862677457482],\n",
       "  [0.9996309039196107],\n",
       "  [0.9995264294131696],\n",
       "  [1.0000849842407997],\n",
       "  [1.000418046813352],\n",
       "  [1.0003585873382281],\n",
       "  [1.0159797041664016],\n",
       "  [-0.9997424743869764],\n",
       "  [1.069098035042501],\n",
       "  [1.0495423308740184],\n",
       "  [0.9999270049054543],\n",
       "  [0.9998452734325143],\n",
       "  [1.0004921577543868],\n",
       "  [0.9999554507983278],\n",
       "  [0.9999406038300971],\n",
       "  [0.9999573822020991],\n",
       "  [1.2190160845144933],\n",
       "  [1.000071178012707],\n",
       "  [0.9997583681842712],\n",
       "  [1.0003542889926285],\n",
       "  [1.2260826988682871],\n",
       "  [1.0001728131037328],\n",
       "  [1.000000036982475],\n",
       "  [1.1488377799504839],\n",
       "  [1.0002322671025108],\n",
       "  [1.030730135408965],\n",
       "  [0.9996989430164591],\n",
       "  [0.9998950766422654],\n",
       "  [1.184522035672418],\n",
       "  [0.9999390019172745],\n",
       "  [1.000252354667623],\n",
       "  [0.9994973817798034],\n",
       "  [0.9997394777613957],\n",
       "  [1.0001720699493455],\n",
       "  [1.5242699877092858],\n",
       "  [1.04239538053137],\n",
       "  [1.02863496630783],\n",
       "  [0.999942841356092],\n",
       "  [0.9999742392168944],\n",
       "  [1.06592529552459],\n",
       "  [1.000282449331136],\n",
       "  [1.628607470156826],\n",
       "  [1.2177928121616741],\n",
       "  [0.9995146008940856],\n",
       "  [1.0874814901212462],\n",
       "  [1.0170881303420858],\n",
       "  [1.165259435666119],\n",
       "  [1.000307364147377],\n",
       "  [0.999996591509097],\n",
       "  [1.1038982801849846],\n",
       "  [0.9996477461149258],\n",
       "  [1.2027992476432623],\n",
       "  [1.047639901426082],\n",
       "  [1.158798019044305],\n",
       "  [1.0760919984192627],\n",
       "  [1.342355256547291],\n",
       "  [-1.0003377284583364],\n",
       "  [1.030125467794752],\n",
       "  [1.1093591842347343],\n",
       "  [1.21813386354995],\n",
       "  [-1.0002286660929762],\n",
       "  [0.999794004134041],\n",
       "  [-1.399487594813364],\n",
       "  [1.141339209813852],\n",
       "  [1.000210222211213],\n",
       "  [1.3701342901124631],\n",
       "  [1.2037792169687036],\n",
       "  [1.1416445555141623],\n",
       "  [-1.0000169718890501],\n",
       "  [-1.0003848668139632],\n",
       "  [-0.999778760588829]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from svmutil import *\n",
    "svm_model.predict = lambda self, x: svm_predict([0], [x], self)[0][0]\n",
    "\n",
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "trainMat=[]\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "\n",
    "prob = svm_problem(listClasses, trainMat)\n",
    "\n",
    "param = svm_parameter()\n",
    "param.kernel_type = LINEAR\n",
    "param.C = 10\n",
    "\n",
    "m=svm_train(prob, param)\n",
    "\n",
    "# m.predict([3,1])\n",
    "svm_predict(listClasses, trainMat, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6. 결론\n",
    "\n",
    "#### 6.1 NaiveBayesian 분류\n",
    "- Train Data 내의 모든 Data를 100% 정확하게 분류함\n",
    "- 어휘를 Vector로 변환하여 Test 데이터로 넣기가 수월함\n",
    "\n",
    "#### 6.2 SVM (LIBSVM) 분류\n",
    "- Train Data 내의 모든 Data를 100% 정확하게 분류함\n",
    "- 어휘를 Test 데이터로 넣기가 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 문제점\n",
    "\n",
    "### 1. 한글 코딩하기 정~~~~~~~~~~~~~~~~~~~~말 힘듦\n",
    "### 2. 아래 코드를 사용하니 print가 inline으로 안되고 콘솔창에 찍힘\n",
    "> import sys <br>\n",
    "> reload(sys) <br>\n",
    "> sys.setdefaultencoding('utf-8') <br>\n",
    "\n",
    "\n",
    "\n",
    "# 한 학기 동안 고생하셨습니다. 감사합니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
