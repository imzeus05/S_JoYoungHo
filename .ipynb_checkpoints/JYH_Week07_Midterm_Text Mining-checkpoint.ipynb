{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 텍스트마이닝 과제 진행\n",
    ">- 웹에 있는 텍스트에서 정서를 뽑아내는 것\n",
    ">- 중간까지 하나 기말까지 하나\n",
    ">- 주제 : 정서분류 --> 긍정, 부정 or 감정\n",
    ">- 한국문화, 한류 등\n",
    ">- 4/26(화) 까지 Github에 업로드\n",
    ">  - 데이터\n",
    ">  - 보고서 - 가설, 연구방법, 결론\n",
    ">  - 실행프로그램 (책에 있는 내용으로)\n",
    "> - 데이터수집\n",
    ">  - 웹에서 300 doc (300 문장)\n",
    ">  - 테스트와 트레인으로 나눠서 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트마이닝으로 정서분류\n",
    "#### 정서란? \n",
    "- 사람의 마음에 일어나는 여러 가지 감정 또는 감정을 불러일으키는 기분이나 분위기\n",
    "\n",
    "#### 한국문화, 한류 등의 주제로 긍정 or 부정 등의 정서 분류 하기\n",
    "\n",
    "- 연구가설 : 한국 사람들의 한류에 대한 생각이 긍정적 일 것이다.\n",
    "- 연구방법 : 네이버 블로그, 다음 블로그 내용 크롤링 해서 긍정, 부정 분류\n",
    "- 결론 : 한국 사람들의 한류에 대한 생각이 89% 정도가 긍정적이다. (300개 중 267개)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 긍정 데이터 수집\n",
    "#### 키워드 : 한류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python2.7\n",
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "def spider():\n",
    "\n",
    "    with open('python_week07/urls_p.txt') as input_file:\n",
    "        for i, line in enumerate(input_file):\n",
    "            url_info = line.split('\\t')\n",
    "            \n",
    "            url_id = url_info[0]\n",
    "            url = url_info[1]\n",
    "            selector = url_info[2]\n",
    "            \n",
    "            source_code = requests.get(url)\n",
    "            plain_text = source_code.text\n",
    "\n",
    "            soup = BeautifulSoup(plain_text, 'lxml')\n",
    "            \n",
    "            final_text = \"\"\n",
    "            file_name = (\"python_week07/doc_p/%s.txt\" % url_id)\n",
    "            out = open(file_name, 'w')\n",
    "            for contents in soup.select(selector):\n",
    "                try:\n",
    "                    final_text += str(unicode(contents.text))\n",
    "                    #final_text += contents.text\n",
    "                except UnicodeEncodeError:\n",
    "                    pass\n",
    "\n",
    "            print >> out, final_text\n",
    "            #print(final_text.encode('utf-8').decode('utf-8'))\n",
    "            out.close()            \n",
    "            print(url_id + ': ' + url)\n",
    "\n",
    "spider()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. 부정 데이터 수집\n",
    "#### 키워드 : 한류 사기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python2.7\n",
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "def spider():\n",
    "\n",
    "    with open('python_week07/urls_n.txt') as input_file:\n",
    "        for i, line in enumerate(input_file):\n",
    "            url_info = line.split('\\t')\n",
    "            \n",
    "            url_id = url_info[0]\n",
    "            url = url_info[1]\n",
    "            selector = url_info[2]\n",
    "            \n",
    "            source_code = requests.get(url)\n",
    "            plain_text = source_code.text\n",
    "\n",
    "            soup = BeautifulSoup(plain_text, 'lxml')\n",
    "            \n",
    "            final_text = \"\"\n",
    "            file_name = (\"python_week07/doc_rss/%s.txt\" % url_id)\n",
    "            out = open(file_name, 'w')\n",
    "            for contents in soup.select(selector):\n",
    "                try:\n",
    "                    final_text += str(unicode(contents.text))\n",
    "                    #final_text += contents.text\n",
    "                except UnicodeEncodeError:\n",
    "                    pass\n",
    "\n",
    "            print >> out, final_text\n",
    "            #print(final_text.encode('utf-8').decode('utf-8'))\n",
    "            out.close()\n",
    "            print(url_id + ': ' + url)\n",
    "\n",
    "spider()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 잠깐!\n",
    "## 위 1, 2의 방법은 시간이 너무 많이 걸려서 RSS 이용으로 방법을 바꿈\n",
    "\n",
    "### 1. 데이터 수집 : RSS 이용\n",
    "- 네이버 블로그, 다음 블로그 중심으로\n",
    "- 키워드 : 한류, 반한류, 혐한류\n",
    "- 키워드로 검색후 나온 페이지의 rss 주소를 urls_rss.txt 에 저장 (210개의 rss 주소 수집)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python2.7\n",
    "# -*- coding: utf-8 -*-\n",
    "import feedparser\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "def spider():\n",
    "    \n",
    "    with open('python_week07/urls_rss.txt') as input_file:\n",
    "        \n",
    "        count = 0\n",
    "        keyword = '한류'\n",
    "        \n",
    "        for i, line in enumerate(input_file):\n",
    "\n",
    "            url_info = line.split('\\t')\n",
    "            url = url_info[0]\n",
    "            rss = feedparser.parse(url)\n",
    "\n",
    "            for post in rss.entries:\n",
    "                final_text = \"\"\n",
    "                final_text = post.description\n",
    "                if(keyword in final_text):\n",
    "                    count = count + 1\n",
    "                    file_name = (\"python_week07/doc_rss/%s.txt\" % count)\n",
    "                    out = open(file_name, 'w')\n",
    "                    #print(final_text)\n",
    "                    print >> out, final_text\n",
    "                    out.close()\n",
    "                    \n",
    "spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. 데이터 분류 : 수작업\n",
    "- 수집된 300개의 데이터를 내용을 보고 긍정, 부정으로 분류\n",
    "- 긍정은 doc_rss>p 폴더, 부정은 doc_rss>n 폴더"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. NLP\n",
    "- NLP는 대표적인 한글 파이썬 NLP인 konlpy 이용\n",
    "- 분류된 텍스트를 NLP를 이용해 명사, 동사, 형용사 위주로 분리\n",
    "- 텍스트 분리후 doc_nlp.txt 에 한꺼번에 저장 (긍정은 1, 부정은 0 으로 태깅. 267개:33개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python2.7\n",
    "# -*- coding: utf-8 -*-\n",
    "import konlpy\n",
    "import nltk\n",
    "import os.path\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "def nlp():\n",
    "    \n",
    "    # Define a chunk grammar, or chunking rules, then chunk\n",
    "    grammar = \"\"\"\n",
    "    NNG: {<NNG.*>*}   # Noun phrase\n",
    "    NNP: {<NNP.*>*}   # Noun phrase\n",
    "    NP: {<NP.*>*}     # Noun phrase\n",
    "    VV: {<VV.*>*}     # Verb phrase\n",
    "    VA: {<VA.*>*}     # Adjective phrase\n",
    "    \"\"\"\n",
    "\n",
    "    # 초기화\n",
    "    file_name = \"python_week07/doc_rss/doc_nlp.txt\"\n",
    "    open(file_name, 'w').close()\n",
    "    \n",
    "    for no in range(1, 301):\n",
    "    \n",
    "        txt_file = 'python_week07/doc_rss/p/%s.txt' % no\n",
    "        if(os.path.isfile(txt_file)):\n",
    "            pn_type = '1'\n",
    "        else:\n",
    "            txt_file = 'python_week07/doc_rss/n/%s.txt' % no\n",
    "            pn_type = '0'\n",
    "\n",
    "        with open(txt_file) as input_file:\n",
    "\n",
    "            sentence = \"\"\n",
    "            for i, line in enumerate(input_file):\n",
    "                sentence = sentence + ' ' + line\n",
    "\n",
    "            words = konlpy.tag.Kkma().pos(sentence)\n",
    "\n",
    "            #print(words)\n",
    "            parser = nltk.RegexpParser(grammar)\n",
    "            chunks = parser.parse(words)\n",
    "\n",
    "            #word_arr = set([])\n",
    "            word_arr = pn_type + \"\\t\"\n",
    "            for subtree in chunks.subtrees():\n",
    "                if subtree.label()=='NNG' or subtree.label()=='NNP' or subtree.label()=='NP' or subtree.label()=='VV' or subtree.label()=='VA':\n",
    "                    for sub in subtree:\n",
    "                        try:\n",
    "                            #word_arr = word_arr | set([sub.__getitem__(0)])\n",
    "                            word_arr = word_arr + sub.__getitem__(0) + ' '\n",
    "                        except UnicodeEncodeError:\n",
    "                            pass\n",
    "\n",
    "            out = open(file_name, 'a')\n",
    "            #print(unicode(word_arr))\n",
    "            print >> out, word_arr\n",
    "            out.close()\n",
    "        \n",
    "nlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. 단어 벡터 만들기\n",
    "- train set\n",
    "- test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python2.7\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "\n",
    "def loadDataSet():\n",
    "    postingList = []\n",
    "    classVec = []\n",
    "    \n",
    "    with open('python_week07/doc_rss/doc_nlp.txt') as input_file:\n",
    "        \n",
    "        for i, line in enumerate(input_file):\n",
    "\n",
    "            info = line.split('\\t')\n",
    "            pos_neg = info[0]\n",
    "            text = info[1]\n",
    "            \n",
    "            postingList.append(text.split())\n",
    "            classVec.append(int(pos_neg))\n",
    "            \n",
    "    return postingList,classVec\n",
    "                 \n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print \"the word: %s is not in my Vocabulary!\" % word\n",
    "    return returnVec\n",
    "\n",
    "\n",
    "\n",
    "# listOPosts, listClasses = loadDataSet()\n",
    "# myVocabList = createVocabList(listOPosts)\n",
    "# print unicode(myVocabList[0])\n",
    "# #print setOfWords2Vec(myVocabList, listOPosts[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 훈련 : 단어 벡터로 확률 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python2.7\n",
    "# -*- coding: utf-8 -*-\n",
    "from numpy import *\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) #+ log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) #+ log(1.0 - pClass1)\n",
    "    \n",
    "    print('p1: %s, p0: %s' % (p1, p0))\n",
    "    \n",
    "    if p1 > p0:\n",
    "        return u'긍정'\n",
    "    else: \n",
    "        return u'부정'\n",
    "    \n",
    "    \n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "        \n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = zeros(numWords); p1Num = zeros(numWords)      #change to ones() \n",
    "    p0Denom = 0.0; p1Denom = 0.0                        #change to 2.0\n",
    "#     p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() \n",
    "#     p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "#     p1Vect = log(p1Num/p1Denom)          #change to log()\n",
    "#     p0Vect = log(p0Num/p0Denom)          #change to log()\n",
    "    p1Vect = p1Num/p1Denom          #change to log()\n",
    "    p0Vect = p0Num/p0Denom          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 최종 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python2.7\n",
    "# -*- coding: utf-8 -*-\n",
    "import feedparser\n",
    "import konlpy\n",
    "import nltk\n",
    "from numpy import *\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "\n",
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    \n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "        \n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "        \n",
    "#     print pAb\n",
    "#     print p0V\n",
    "#     print p1V    \n",
    "    \n",
    "    \n",
    "    # Define a chunk grammar, or chunking rules, then chunk\n",
    "    grammar = \"\"\"\n",
    "    NNG: {<NNG.*>*}   # Noun phrase\n",
    "    NNP: {<NNP.*>*}   # Noun phrase\n",
    "    NP: {<NP.*>*}     # Noun phrase\n",
    "    VV: {<VV.*>*}     # Verb phrase\n",
    "    VA: {<VA.*>*}     # Adjective phrase\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # 테스트할 문장\n",
    "    \n",
    "    ## 긍정\n",
    "    rss = feedparser.parse('http://blog.rss.naver.com/myth2030.xml')\n",
    "    ## 부정\n",
    "#     rss = feedparser.parse('http://blog.rss.naver.com/golf798kang5.xml')\n",
    "\n",
    "    for post in rss.entries:\n",
    "        final_text = \"\"\n",
    "        final_text = post.description\n",
    "        \n",
    "        if('한류' in final_text):\n",
    "            sentence = unicode(final_text)\n",
    "#             sentence = u\"방 중국 어제 보 중국인 무감각 표정 맛 멋 없 언행 나 이번 방 미사일 초토화되 중국 미소 지켜보 태양 후예 드라마 황 치열 가수 어떠 사람 말하 한류 중국 동남아 공산권 회교권 강타 이유 감성 차 하 맞 말 보 공산주의 유물론 젖 영혼 물질 팔아먹 중국 강압적 교리 묶이 본성 허리띠 묶이 회교권 향하 날아가 한류 미사일 핵폭탄 퍼지 수년 전 하 상상 현상 작가 나 뛰어나 감성 배우 감성 연기 중국 녹 회교권 다운 이 세기 감성 시대 사실 웅변 거대 경제력 엄청나 외환 보유고 거기 다그 각종 첨단 무기 우주 개발 기술 전 세계 뒤덮 같 중국 한국 드라마 하나 한류 가수 의하 대륙 들끓 정부 간섭 하 정도 어디 둘 드라마 흉내 내서 치킨 맥주 잔 마시 장면 연출 위하 한국 새카맣 몰려오 나 그만큼 감성 후지 약하 그 중국 최대 약점 아킬레스건 유구 역사 속 문화 예술 유전 인자 가지 그 공산주의 유물론 치 정신문화 후진국 되 거기 비하 복음 서양 선교사 전하 받 하 복음 인하 나라 밝 되 발전 발전 하 되 주 초가 되 지금 감성 문화 발달 저변 복음 전파 교회 공로 가 지대 말하 있 계몽 시대 지나 음 표현 노래 문화 교회 이끌 하 과언 열매 사회 저변 깔리 국가 전체 감성 흐름 주도 말하 있 정 보도 기술 세계 평준화 되 앞서 뒤서 하 중국 재벌 영화 촬영 세트 미국 할리우드 크 짓 떠들 중국 그렇 하 그러 영혼 노래 감성 흐르 표정 언어 없 연출자 배우 우리 같 드라마 영화 기대 있 그 숙제 되 가수 드라마 그 가슴 울리 웃기 하 나 이번 방 핵 미 사실 같 한류 폭탄 맞 중국 보 그 쉽 생각 하 보 세계 최대 경제 대국 인구 가지 중국 우리 앞 마당 있 장터 느낌 지우 없 \"\n",
    "            words = konlpy.tag.Kkma().pos(sentence)\n",
    "\n",
    "            #print(words)\n",
    "            parser = nltk.RegexpParser(grammar)\n",
    "            chunks = parser.parse(words)\n",
    "\n",
    "            #word_arr = set([])\n",
    "            word_arr = \"\"\n",
    "            for subtree in chunks.subtrees():\n",
    "                if subtree.label()=='NNG' or subtree.label()=='NNP' or subtree.label()=='NP' or subtree.label()=='VV' or subtree.label()=='VA':\n",
    "                    for sub in subtree:\n",
    "                        try:\n",
    "                            #word_arr = word_arr | set([sub.__getitem__(0)])\n",
    "                            word_arr = word_arr + sub.__getitem__(0) + ' '\n",
    "                        except UnicodeEncodeError:\n",
    "                            pass\n",
    "\n",
    "            #print(word_arr)\n",
    "            testEntry = word_arr.split()\n",
    "            thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "            print sentence,'\\n classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "    \n",
    "    \n",
    "testingNB()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 문제점\n",
    "\n",
    "### 1. 한글 코딩하기 정~~~~~~~~~~~~~~~~~~~~말 힘들\n",
    "### 2. 아래 코드를 사용하니 print가 inline으로 안되고 콘솔에 찍힘\n",
    "> import sys <br>\n",
    "> reload(sys) <br>\n",
    "> sys.setdefaultencoding('utf-8') <br>\n",
    "\n",
    "\n",
    "\n",
    "# 끝!!!!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
