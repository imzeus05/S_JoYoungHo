{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4장. 나이브 베이스: 확률 이론으로 분류하기\n",
    "\n",
    "#### 베이시안은 명목 데이터를 주로 사용하지만 연속형 데이터를 평균, 분산을 구해 가우시안 모델을 적용해 사용하기도 함\n",
    "#### 사전 확률이 중요하다.\n",
    "\n",
    "### NLP 에서 불필요한 문자 빼주기 \n",
    "##### stopword\n",
    "##### wordvector = bag of words\n",
    "##### wordvector는 보통 1000개 이상 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# loadDataSet(): 단어 리스트와 리스트안의 문서가 폭력적인지(0) 폭력적이지 않은지(1) 분류 값을 반환\n",
    "def loadDataSet():\n",
    "    postingList=[['my','dog','has','flea','problems','help','please'],\n",
    "                 ['maybe','not','take','him','to','dog','park','stupid'],\n",
    "                ['my','dalmation','is','so','cute','I','love','him'],\n",
    "                ['stop','posting','stupid','worthless','garbage'],\n",
    "                ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "                ['quit','buying','worthless','dog','food','stupid']]\n",
    "    classVec = [0,1,0,1,0,1] # 1: 폭력적인 0: 폭력적이지 않음\n",
    "    return postingList,classVec\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print \"the word: %s is not in my Vocabulary!\" % word\n",
    "    return returnVec\n",
    "\n",
    "\n",
    "\n",
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "print myVocabList\n",
    "\n",
    "print setOfWords2Vec(myVocabList, listOPosts[0])\n",
    "print setOfWords2Vec(myVocabList, listOPosts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0], [1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]]\n",
      "[ 0.07692308  0.07692308  0.07692308  0.03846154  0.03846154  0.07692308\n",
      "  0.07692308  0.07692308  0.03846154  0.07692308  0.07692308  0.07692308\n",
      "  0.07692308  0.03846154  0.03846154  0.11538462  0.03846154  0.03846154\n",
      "  0.07692308  0.03846154  0.07692308  0.07692308  0.03846154  0.07692308\n",
      "  0.07692308  0.07692308  0.03846154  0.07692308  0.03846154  0.07692308\n",
      "  0.07692308  0.15384615]\n",
      "[ 0.04761905  0.04761905  0.04761905  0.0952381   0.0952381   0.04761905\n",
      "  0.04761905  0.04761905  0.0952381   0.0952381   0.04761905  0.04761905\n",
      "  0.04761905  0.0952381   0.0952381   0.0952381   0.0952381   0.0952381\n",
      "  0.04761905  0.14285714  0.04761905  0.0952381   0.0952381   0.04761905\n",
      "  0.14285714  0.04761905  0.19047619  0.04761905  0.0952381   0.04761905\n",
      "  0.04761905  0.04761905]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0]) ## 각 분류 항목에 대한 문서의 개수 세기\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs) #사전확률 계산 (폭력적이지 않은(1)에 대한)\n",
    "#     p0Num = zeros(numWords); p1Num = zeros(numWords) ##초기화\n",
    "#     p0Denom = 0.0; p1Denom = 0.0 \n",
    "    p0Num = ones(numWords); p1Num = ones(numWords) ##초기화\n",
    "    p0Denom = 2.0; p1Denom = 2.0 \n",
    "    for i in range(numTrainDocs): ## 훈련을 위한 모든 문서의 개수만큼 반복\n",
    "        if trainCategory[i] ==1: ## 폭력적인 단어의 문서일 경우\n",
    "            p1Num +=trainMatrix[i] ## 해당문서의 단어 갯수 증가\n",
    "            p1Denom += sum(trainMatrix[i]) ## 전체 문서에서 해당 문서의 단어 갯수\n",
    "        else: ## 비폭력적인 단어의 문서일 경우\n",
    "            p0Num += trainMatrix[i]  ## 해당문서의 단어 갯수 증가\n",
    "            p0Denom += sum(trainMatrix[i])## 전체 문서에서 해당 문서의 단어 갯수\n",
    "#     p1Vect = p1Num / p1Denom ## 각 폭력적인 단어들이 전체 폭력적인 단어에서 나타날 확률\n",
    "#     p0Vect = p0Num / p0Denom ## 각 비폭력적인 단어들이 전체 비폭력적인 단어에서 나타날 확률\n",
    "    p1Vect = p1Num / p1Denom ## 각 폭력적인 단어들이 전체 폭력적인 단어에서 나타날 확률\n",
    "    p0Vect = p0Num / p0Denom ## 각 비폭력적인 단어들이 전체 비폭력적인 단어에서 나타날 확률\n",
    "        \n",
    "    \n",
    "    return p0Vect,p1Vect,pAbusive\n",
    "\n",
    "trainMat=[]\n",
    "for postingDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,postingDoc)) ## 각 5개의 문서의 단어가 전체 단어에서의 위치 혹은 존재 여부\n",
    "print trainMat\n",
    "    \n",
    "p0V,p1V,pAb=trainNB0(trainMat,listClasses)\n",
    "print p0V\n",
    "print p1V\n",
    "print pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n",
      "the word: bad is not in my Vocabulary!\n",
      "['dog', 'cute', 'bad'] classified as:  1\n",
      "['dog', 'cute'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "# 더하는 이유는 0일 경우 0이 되버림, 로그를 취하는 이유는 값이 작을 경우 언더 플로우 나게 됨으로\n",
    "    \n",
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "    testEntry = ['dog', 'cute', 'bad']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "    testEntry = ['dog', 'cute']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "    \n",
    "    \n",
    "## 중복단어 문서 모델로 변경\n",
    "## 해당 단어가 있냐 없냐로 표현하는것을 count 로 변경\n",
    "## 많이 등장하는 단어는 확률 증가\n",
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec\n",
    "\n",
    "testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
