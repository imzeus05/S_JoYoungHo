{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 나이브 베이스\n",
    "\n",
    "### 나이브 베이스에 대한 접근 방법\n",
    "1. 수집 : RSS 자료\n",
    "2. 준비 : 명목형, 부울형\n",
    "3. 분석 : 히스토그램으로 분석\n",
    "4. 훈련 : 각 속성을 독립적으로 조건부 확률 계산\n",
    "5. 검사 : 오류율 검사\n",
    "6. 사용 : 문서 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이썬으로 텍스트 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "0.5\n",
      "[-2.56494936 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936\n",
      " -2.56494936 -3.25809654 -3.25809654 -2.15948425 -3.25809654 -3.25809654\n",
      " -2.56494936 -3.25809654 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -1.87180218]\n",
      "[-3.04452244 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244 -3.04452244\n",
      " -3.04452244 -2.35137526 -2.35137526 -2.35137526 -2.35137526 -2.35137526\n",
      " -3.04452244 -1.94591015 -3.04452244 -2.35137526 -2.35137526 -3.04452244\n",
      " -1.94591015 -3.04452244 -1.65822808 -3.04452244 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244]\n",
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "### 1. 준비 : 텍스트로 단어 벡터 만들기\n",
    "\n",
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec\n",
    "                 \n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print \"the word: %s is not in my Vocabulary!\" % word\n",
    "    return returnVec\n",
    "\n",
    "\n",
    "\n",
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "print myVocabList\n",
    "\n",
    "print setOfWords2Vec(myVocabList, list0Posts[0])\n",
    "print setOfWords2Vec(myVocabList, list0Posts[3])\n",
    "\n",
    "\n",
    "### 2. 훈련: 단어 벡터로 확률 계산하기\n",
    "import numpy\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "#     p0Num = zeros(numWords); p1Num = zeros(numWords)\n",
    "#     p0Denom = 0.0; p1Denom = 0.0                    \n",
    "    p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() \n",
    "    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = log(p1Num/p1Denom)          #change to log()\n",
    "    p0Vect = log(p0Num/p0Denom)          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive\n",
    "\n",
    "\n",
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "trainMat=[]\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    \n",
    "    \n",
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "\n",
    "print pAb\n",
    "print p0V\n",
    "print p1V\n",
    "# 책과 p0V, p1V 결과가 다름\n",
    "\n",
    "### 3. 검사: 실제 조건을 반영하기 위해 분류기 수정하기\n",
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "    \n",
    "testingNB()\n",
    "\n",
    "\n",
    "### 4. 준비: 중복 단어 문서 모델\n",
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제: 스팸 이메일 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M.L.', 'I', 'have', 'ever', 'laid', 'eyes', 'upon']\n",
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon']\n",
      "['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'm', 'l', 'i', 'have', 'ever', 'laid', 'eyes', 'upon']\n",
      "['Hello', 'Since', 'you', 'are', 'an', 'owner', 'of', 'at', 'least', 'one', 'Google', 'Groups', 'group', 'that', 'uses', 'the', 'customized', 'welcome', 'message', 'pages', 'or', 'files', 'we', 'are', 'writing', 'to', 'inform', 'you', 'that', 'we', 'will', 'no', 'longer', 'be', 'supporting', 'these', 'features', 'starting', 'February', '2011', 'We', 'made', 'this', 'decision', 'so', 'that', 'we', 'can', 'focus', 'on', 'improving', 'the', 'core', 'functionalities', 'of', 'Google', 'Groups', 'mailing', 'lists', 'and', 'forum', 'discussions', 'Instead', 'of', 'these', 'features', 'we', 'encourage', 'you', 'to', 'use', 'products', 'that', 'are', 'designed', 'specifically', 'for', 'file', 'storage', 'and', 'page', 'creation', 'such', 'as', 'Google', 'Docs', 'and', 'Google', 'Sites', 'For', 'example', 'you', 'can', 'easily', 'create', 'your', 'pages', 'on', 'Google', 'Sites', 'and', 'share', 'the', 'site', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '174623', 'with', 'the', 'members', 'of', 'your', 'group', 'You', 'can', 'also', 'store', 'your', 'files', 'on', 'the', 'site', 'by', 'attaching', 'files', 'to', 'pages', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '90563', 'on', 'the', 'site', 'If', 'you', 're', 'just', 'looking', 'for', 'a', 'place', 'to', 'upload', 'your', 'files', 'so', 'that', 'your', 'group', 'members', 'can', 'download', 'them', 'we', 'suggest', 'you', 'try', 'Google', 'Docs', 'You', 'can', 'upload', 'files', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '50092', 'and', 'share', 'access', 'with', 'either', 'a', 'group', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '66343', 'or', 'an', 'individual', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '86152', 'assigning', 'either', 'edit', 'or', 'download', 'only', 'access', 'to', 'the', 'files', 'you', 'have', 'received', 'this', 'mandatory', 'email', 'service', 'announcement', 'to', 'update', 'you', 'about', 'important', 'changes', 'to', 'Google', 'Groups', '']\n",
      "classification error ['ryan', 'whybrew', 'commented', 'your', 'status', 'ryan', 'wrote', 'turd', 'ferguson', 'butt', 'horn']\n",
      "the error rate is:  0.1\n",
      "the error rate is:  0.0\n",
      "the error rate is:  0.0\n"
     ]
    }
   ],
   "source": [
    "#import python_week05.bayes\n",
    "from python_week05 import bayes\n",
    "import numpy\n",
    "\n",
    "### 1. 준비: 텍스트 토큰 만들기\n",
    "mySent='This book is the best book on Python or M.L. I have ever laid eyes upon'\n",
    "print mySent.split()\n",
    "\n",
    "import re\n",
    "regEx = re.compile('\\\\W*')\n",
    "listOfTokens = regEx.split(mySent)\n",
    "print listOfTokens\n",
    "# 책과 결과가 다름 (공백 없는 데이터가 없음)\n",
    "\n",
    "print [tok.lower() for tok in listOfTokens if len(tok) > 0]\n",
    "\n",
    "emailText = open('python_week05/email/ham/6.txt').read()\n",
    "listOfTokens = regEx.split(emailText)\n",
    "print listOfTokens\n",
    "\n",
    "### 2. 검사: 나이브 베이스로 교차 검증하기\n",
    "def textParse(bigString):    #input is big string, #output is word list\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] \n",
    "    \n",
    "def spamTest():\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open('python_week05/email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('python_week05/email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = bayes.createVocabList(docList)#create vocabulary\n",
    "    trainingSet = range(50); testSet=[]           #create test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(numpy.random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bayes.bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = bayes.trainNB0(numpy.array(trainMat),numpy.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bayes.bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if bayes.classifyNB(numpy.array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print \"classification error\",docList[docIndex]\n",
    "    print 'the error rate is: ',float(errorCount)/len(testSet)\n",
    "    #return vocabList,fullText\n",
    "    \n",
    "\n",
    "spamTest()\n",
    "spamTest()\n",
    "spamTest()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 예제: 개인 광고에 포함된 지역 특색 도출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.35\n",
      "the error rate is:  0.5\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "only\n",
      "all\n",
      "try\n",
      "what\n",
      "men\n",
      "experience\n",
      "one\n",
      "sex\n",
      "movie\n",
      "fun\n",
      "nice\n",
      "young\n",
      "friendly\n",
      "woman\n",
      "very\n",
      "zero\n",
      "evening\n",
      "new\n",
      "may\n",
      "after\n",
      "maybe\n",
      "out\n",
      "little\n",
      "platonic\n",
      "relationship\n",
      "don\n",
      "going\n",
      "get\n",
      "watch\n",
      "been\n",
      "except\n",
      "tax\n",
      "has\n",
      "hey\n",
      "stay\n",
      "newbie\n",
      "dance\n",
      "shot\n",
      "mon\n",
      "hate\n",
      "buddy\n",
      "dislike\n",
      "choice\n",
      "spoon\n",
      "decide\n",
      "enjoy\n",
      "licenses\n",
      "click\n",
      "even\n",
      "barbie\n",
      "tattoos\n",
      "outgoing\n",
      "assertive\n",
      "change\n",
      "involved\n",
      "amount\n",
      "makes\n",
      "love\n",
      "secure\n",
      "indoors\n",
      "crime\n",
      "more\n",
      "teen\n",
      "company\n",
      "hold\n",
      "town\n",
      "movies\n",
      "cuddle\n",
      "nine\n",
      "suggestions\n",
      "states\n",
      "something\n",
      "honest\n",
      "huge\n",
      "needs\n",
      "watching\n",
      "how\n",
      "pizza\n",
      "okay\n",
      "mac\n",
      "lay\n",
      "guys\n",
      "cuddling\n",
      "tall\n",
      "talk\n",
      "refund\n",
      "over\n",
      "years\n",
      "personal\n",
      "might\n",
      "x__o\n",
      "food\n",
      "day\n",
      "glasses\n",
      "financial\n",
      "doing\n",
      "connect\n",
      "sexually\n",
      "girl\n",
      "inquires\n",
      "blackk\n",
      "free\n",
      "worries\n",
      "california\n",
      "ask\n",
      "partner\n",
      "could\n",
      "feel\n",
      "number\n",
      "fancy\n",
      "lol\n",
      "adventitious\n",
      "ptin\n",
      "snuggling\n",
      "night\n",
      "anyone\n",
      "ctec\n",
      "returns\n",
      "too\n",
      "part\n",
      "bed\n",
      "venture\n",
      "chest\n",
      "take\n",
      "though\n",
      "sometimes\n",
      "particularly\n",
      "text\n",
      "cheap\n",
      "outside\n",
      "athletic\n",
      "circle\n",
      "hope\n",
      "feels\n",
      "breast\n",
      "ema\n",
      "pimp\n",
      "shy\n",
      "taxes\n",
      "stuff\n",
      "arm\n",
      "best\n",
      "arou\n",
      "visiting\n",
      "away\n",
      "between\n",
      "missing\n",
      "last\n",
      "curly\n",
      "joke\n",
      "alot\n",
      "simple\n",
      "laugh\n",
      "fri\n",
      "much\n",
      "wants\n",
      "life\n",
      "else\n",
      "chill\n",
      "look\n",
      "choices\n",
      "while\n",
      "seven\n",
      "piercings\n",
      "ready\n",
      "things\n",
      "fling\n",
      "same\n",
      "largest\n",
      "unconventional\n",
      "emotionally\n",
      "hang\n",
      "lower\n",
      "older\n",
      "couch\n",
      "yet\n",
      "reasons\n",
      "easy\n",
      "smart\n",
      "around\n",
      "big\n",
      "dark\n",
      "know\n",
      "lady\n",
      "apart\n",
      "specific\n",
      "overrated\n",
      "because\n",
      "deal\n",
      "some\n",
      "hair\n",
      "curious\n",
      "legal\n",
      "planni\n",
      "420\n",
      "snuggle\n",
      "patient\n",
      "business\n",
      "gentleman\n",
      "burner\n",
      "exciting\n",
      "anything\n",
      "drama\n",
      "own\n",
      "into\n",
      "year\n",
      "mba\n",
      "parties\n",
      "drifted\n",
      "strictly\n",
      "long\n",
      "accordingly\n",
      "way\n",
      "lowest\n",
      "tight\n",
      "head\n",
      "offering\n",
      "deductions\n",
      "also\n",
      "attached\n",
      "flirt\n",
      "email\n",
      "home\n",
      "scribe\n",
      "income\n",
      "scrub\n",
      "friends\n",
      "longer\n",
      "dvd\n",
      "together\n",
      "curvy\n",
      "back\n",
      "far\n",
      "serious\n",
      "consider\n",
      "crossdressing\n",
      "relieve\n",
      "focus\n",
      "resourceful\n",
      "whose\n",
      "technique\n",
      "send\n",
      "charge\n",
      "women\n",
      "activities\n",
      "straightforward\n",
      "sitting\n",
      "every\n",
      "minded\n",
      "cook\n",
      "difference\n",
      "cool\n",
      "school\n",
      "level\n",
      "loneliness\n",
      "compassionate\n",
      "tissue\n",
      "discrete\n",
      "sane\n",
      "melodies\n",
      "guy\n",
      "ten\n",
      "tired\n",
      "video\n",
      "further\n",
      "drinks\n",
      "sub\n",
      "current\n",
      "issues\n",
      "exercise\n",
      "satisfy\n",
      "full\n",
      "exchange\n",
      "hours\n",
      "explore\n",
      "protection\n",
      "let\n",
      "groups\n",
      "alone\n",
      "great\n",
      "kids\n",
      "host\n",
      "prior\n",
      "smoke\n",
      "social\n",
      "had\n",
      "suitable\n",
      "massage\n",
      "genuinely\n",
      "semester\n",
      "tap\n",
      "crisis\n",
      "from\n",
      "lube\n",
      "sports\n",
      "few\n",
      "live\n",
      "call\n",
      "tell\n",
      "posting\n",
      "relax\n",
      "successful\n",
      "share\n",
      "musician\n",
      "hypno\n",
      "glass\n",
      "train\n",
      "excellent\n",
      "must\n",
      "keeping\n",
      "hour\n",
      "athlete\n",
      "gasoline\n",
      "cat\n",
      "learn\n",
      "meet\n",
      "male\n",
      "drizzling\n",
      "beautiful\n",
      "kenny\n",
      "give\n",
      "boerum\n",
      "sense\n",
      "dress\n",
      "swedish\n",
      "str8\n",
      "agendas\n",
      "afr\n",
      "tension\n",
      "curiosity\n",
      "plann\n",
      "such\n",
      "types\n",
      "man\n",
      "stress\n",
      "short\n",
      "manga\n",
      "wine\n",
      "cute\n",
      "deck\n",
      "still\n",
      "its\n",
      "perfect\n",
      "fit\n",
      "willing\n",
      "hidden\n",
      "soccer\n",
      "then\n",
      "them\n",
      "return\n",
      "seeking\n",
      "flour\n",
      "workout\n",
      "safe\n",
      "dat\n",
      "lunch\n",
      "they\n",
      "now\n",
      "bollywood\n",
      "possess\n",
      "assess\n",
      "name\n",
      "always\n",
      "fitness\n",
      "friendship\n",
      "weight\n",
      "our\n",
      "sexual\n",
      "thick\n",
      "special\n",
      "alcohol\n",
      "rubdown\n",
      "hill\n",
      "got\n",
      "assured\n",
      "quite\n",
      "existentialist\n",
      "craving\n",
      "longterm\n",
      "days\n",
      "lotions\n",
      "thing\n",
      "isn\n",
      "loud\n",
      "interact\n",
      "south\n",
      "yourself\n",
      "date\n",
      "done\n",
      "another\n",
      "communicator\n",
      "owner\n",
      "ring\n",
      "open\n",
      "fire\n",
      "city\n",
      "guess\n",
      "their\n",
      "top\n",
      "master\n",
      "white\n",
      "toe\n",
      "interests\n",
      "gym\n",
      "wisdom\n",
      "somewhat\n",
      "loving\n",
      "copy\n",
      "boyfriend\n",
      "std\n",
      "professionals\n",
      "kind\n",
      "favors\n",
      "scenes\n",
      "matter\n",
      "classes\n",
      "marriage\n",
      "lover\n",
      "listen\n",
      "ages\n",
      "comfortable\n",
      "tonight\n",
      "talking\n",
      "say\n",
      "need\n",
      "seek\n",
      "any\n",
      "superman\n",
      "afraid\n",
      "genuine\n",
      "built\n",
      "hispanic\n",
      "able\n",
      "mid\n",
      "lik\n",
      "males\n",
      "equipment\n",
      "hookah\n",
      "which\n",
      "online\n",
      "wanting\n",
      "latino\n",
      "blunt\n",
      "play\n",
      "sure\n",
      "sunnyvale\n",
      "most\n",
      "brooklyn\n",
      "regular\n",
      "mouth\n",
      "addict\n",
      "exercises\n",
      "why\n",
      "queens\n",
      "audio\n",
      "dom\n",
      "odds\n",
      "professional\n",
      "skinny\n",
      "typical\n",
      "agreed\n",
      "bring\n",
      "bedroom\n",
      "find\n",
      "busy\n",
      "title\n",
      "unhappy\n",
      "should\n",
      "black\n",
      "pretty\n",
      "money\n",
      "folks\n",
      "local\n",
      "personality\n",
      "regularly\n",
      "hiv\n",
      "photograph\n",
      "bear\n",
      "reply\n",
      "artist\n",
      "married\n",
      "bay\n",
      "release\n",
      "where\n",
      "husband\n",
      "ears\n",
      "body\n",
      "college\n",
      "soothing\n",
      "practice\n",
      "closet\n",
      "currently\n",
      "horny\n",
      "enough\n",
      "calderone\n",
      "across\n",
      "ends\n",
      "latin\n",
      "however\n",
      "job\n",
      "coffee\n",
      "come\n",
      "career\n",
      "taking\n",
      "etc\n",
      "connection\n",
      "sweet\n",
      "scottish\n",
      "5three\n",
      "trust\n",
      "comical\n",
      "blink\n",
      "deep\n",
      "fro\n",
      "treat\n",
      "meeting\n",
      "relaxing\n",
      "drugs\n",
      "mind\n",
      "understand\n",
      "lonely\n",
      "gunpowder\n",
      "three4seven\n",
      "referrals\n",
      "these\n",
      "straight\n",
      "value\n",
      "sexuality\n",
      "leaning\n",
      "engaged\n",
      "non\n",
      "dubious\n",
      "sustenance\n",
      "strain\n",
      "upscale\n",
      "motivated\n",
      "attach\n",
      "different\n",
      "anymore\n",
      "perhaps\n",
      "make\n",
      "gentlemen\n",
      "exhibitionists\n",
      "party\n",
      "entitled\n",
      "week\n",
      "oil\n",
      "novels\n",
      "driven\n",
      "moment\n",
      "student\n",
      "totally\n",
      "expand\n",
      "break\n",
      "practiced\n",
      "youre\n",
      "well\n",
      "person\n",
      "without\n",
      "qualities\n",
      "muscle\n",
      "spend\n",
      "left\n",
      "snowman\n",
      "outdoor\n",
      "being\n",
      "photo\n",
      "actress\n",
      "shape\n",
      "regardless\n",
      "happiness\n",
      "cup\n",
      "seems\n",
      "improvement\n",
      "lets\n",
      "interested\n",
      "location\n",
      "crack\n",
      "ski\n",
      "real\n",
      "couple\n",
      "possible\n",
      "possibly\n",
      "using\n",
      "passable\n",
      "loss\n",
      "intel\n",
      "lost\n",
      "gyms\n",
      "benefit\n",
      "lose\n",
      "hung\n",
      "works\n",
      "old\n",
      "people\n",
      "begin\n",
      "demeanor\n",
      "proper\n",
      "humor\n",
      "normal\n",
      "creative\n",
      "meds\n",
      "does\n",
      "speak\n",
      "holiday\n",
      "rub\n",
      "dental\n",
      "each\n",
      "six0\n",
      "ple\n",
      "working\n",
      "discussed\n",
      "dinner\n",
      "sadistic\n",
      "whats\n",
      "two\n",
      "down\n",
      "nothing\n",
      "weather\n",
      "involvement\n",
      "tame\n",
      "start\n",
      "bored\n",
      "was\n",
      "happy\n",
      "offer\n",
      "jewish\n",
      "adjusting\n",
      "searching\n",
      "goals\n",
      "nassau\n",
      "line\n",
      "romantic\n",
      "limit\n",
      "mature\n",
      "site\n",
      "unusually\n",
      "wicked\n",
      "similar\n",
      "7five3\n",
      "buffs\n",
      "certain\n",
      "pic\n",
      "doesn\n",
      "fellow\n",
      "dear\n",
      "single\n",
      "right\n",
      "chat\n",
      "film\n",
      "physical\n",
      "holla\n",
      "actor\n",
      "lately\n",
      "field\n",
      "other\n",
      "details\n",
      "really\n",
      "picture\n",
      "problems\n",
      "song\n",
      "phone\n",
      "age\n",
      "oral\n",
      "hello\n",
      "songs\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n",
      "all\n",
      "send\n",
      "guy\n",
      "from\n",
      "years\n",
      "gym\n",
      "take\n",
      "black\n",
      "watch\n",
      "some\n",
      "age\n",
      "woman\n",
      "enjoy\n",
      "what\n",
      "full\n",
      "smoke\n",
      "massage\n",
      "male\n",
      "stress\n",
      "maybe\n",
      "cute\n",
      "still\n",
      "day\n",
      "fitness\n",
      "weight\n",
      "out\n",
      "yourself\n",
      "master\n",
      "kind\n",
      "ages\n",
      "need\n",
      "seek\n",
      "any\n",
      "why\n",
      "don\n",
      "professional\n",
      "skinny\n",
      "get\n",
      "body\n",
      "been\n",
      "much\n",
      "life\n",
      "relaxing\n",
      "chill\n",
      "photo\n",
      "real\n",
      "420\n",
      "bored\n",
      "email\n",
      "other\n",
      "nice\n",
      "together\n",
      "crossdressing\n",
      "relieve\n",
      "technique\n",
      "young\n",
      "charge\n",
      "women\n",
      "friendly\n",
      "sitting\n",
      "very\n",
      "minded\n",
      "difference\n",
      "level\n",
      "sane\n",
      "ten\n",
      "drinks\n",
      "even\n",
      "sub\n",
      "issues\n",
      "new\n",
      "exercise\n",
      "exchange\n",
      "hours\n",
      "let\n",
      "alone\n",
      "social\n",
      "tap\n",
      "crisis\n",
      "few\n",
      "live\n",
      "call\n",
      "tell\n",
      "more\n",
      "relax\n",
      "share\n",
      "company\n",
      "hypno\n",
      "must\n",
      "keeping\n",
      "cat\n",
      "cuddle\n",
      "meet\n",
      "give\n",
      "boerum\n",
      "dress\n",
      "str8\n",
      "afr\n",
      "tension\n",
      "types\n",
      "man\n",
      "short\n",
      "manga\n",
      "talk\n",
      "over\n",
      "perfect\n",
      "seeking\n",
      "they\n",
      "now\n",
      "always\n",
      "friendship\n",
      "glasses\n",
      "doing\n",
      "connect\n",
      "our\n",
      "girl\n",
      "sexual\n",
      "special\n",
      "alcohol\n",
      "rubdown\n",
      "hill\n",
      "got\n",
      "assured\n",
      "free\n",
      "existentialist\n",
      "craving\n",
      "could\n",
      "lotions\n",
      "thing\n",
      "isn\n",
      "loud\n",
      "another\n",
      "owner\n",
      "open\n",
      "city\n",
      "guess\n",
      "too\n",
      "toe\n",
      "interests\n",
      "wisdom\n",
      "loving\n",
      "copy\n",
      "boyfriend\n",
      "professionals\n",
      "scenes\n",
      "matter\n",
      "comfortable\n",
      "say\n",
      "afraid\n",
      "genuine\n",
      "built\n",
      "able\n",
      "mid\n",
      "lik\n",
      "equipment\n",
      "hookah\n",
      "wanting\n",
      "latino\n",
      "blunt\n",
      "play\n",
      "sure\n",
      "brooklyn\n",
      "regular\n",
      "queens\n",
      "dom\n",
      "odds\n",
      "typical\n",
      "bring\n",
      "only\n",
      "going\n",
      "money\n",
      "folks\n",
      "hope\n",
      "photograph\n",
      "bear\n",
      "release\n",
      "sex\n",
      "soothing\n",
      "practice\n",
      "closet\n",
      "movie\n",
      "horny\n",
      "enough\n",
      "between\n",
      "ends\n",
      "job\n",
      "come\n",
      "etc\n",
      "sweet\n",
      "blink\n",
      "fro\n",
      "drugs\n",
      "mind\n",
      "referrals\n",
      "look\n",
      "sexuality\n",
      "fun\n",
      "non\n",
      "dubious\n",
      "strain\n",
      "attach\n",
      "different\n",
      "anymore\n",
      "things\n",
      "same\n",
      "exhibitionists\n",
      "oil\n",
      "novels\n",
      "moment\n",
      "totally\n",
      "break\n",
      "older\n",
      "youre\n",
      "well\n",
      "person\n",
      "spend\n",
      "shape\n",
      "regardless\n",
      "yet\n",
      "seems\n",
      "improvement\n",
      "interested\n",
      "location\n",
      "couple\n",
      "possible\n",
      "know\n",
      "using\n",
      "passable\n",
      "loss\n",
      "lost\n",
      "gyms\n",
      "benefit\n",
      "lose\n",
      "because\n",
      "old\n",
      "people\n",
      "begin\n",
      "normal\n",
      "meds\n",
      "rub\n",
      "dental\n",
      "each\n",
      "ple\n",
      "dinner\n",
      "own\n",
      "into\n",
      "two\n",
      "down\n",
      "nothing\n",
      "involvement\n",
      "long\n",
      "was\n",
      "head\n",
      "offering\n",
      "jewish\n",
      "adjusting\n",
      "nassau\n",
      "also\n",
      "romantic\n",
      "limit\n",
      "buffs\n",
      "pic\n",
      "doesn\n",
      "fellow\n",
      "dear\n",
      "film\n",
      "physical\n",
      "lately\n",
      "really\n",
      "problems\n",
      "friends\n",
      "oral\n",
      "hello\n",
      "newbie\n",
      "consider\n",
      "dance\n",
      "focus\n",
      "shot\n",
      "mon\n",
      "hate\n",
      "resourceful\n",
      "whose\n",
      "buddy\n",
      "dislike\n",
      "activities\n",
      "straightforward\n",
      "choice\n",
      "spoon\n",
      "every\n",
      "decide\n",
      "cook\n",
      "cool\n",
      "school\n",
      "loneliness\n",
      "compassionate\n",
      "tissue\n",
      "try\n",
      "discrete\n",
      "melodies\n",
      "tired\n",
      "zero\n",
      "video\n",
      "licenses\n",
      "further\n",
      "click\n",
      "evening\n",
      "barbie\n",
      "current\n",
      "satisfy\n",
      "tattoos\n",
      "outgoing\n",
      "assertive\n",
      "men\n",
      "explore\n",
      "protection\n",
      "groups\n",
      "change\n",
      "great\n",
      "kids\n",
      "involved\n",
      "host\n",
      "experience\n",
      "prior\n",
      "amount\n",
      "had\n",
      "suitable\n",
      "makes\n",
      "genuinely\n",
      "love\n",
      "secure\n",
      "semester\n",
      "indoors\n",
      "crime\n",
      "lube\n",
      "sports\n",
      "teen\n",
      "posting\n",
      "successful\n",
      "musician\n",
      "glass\n",
      "train\n",
      "excellent\n",
      "hold\n",
      "town\n",
      "hour\n",
      "athlete\n",
      "gasoline\n",
      "movies\n",
      "nine\n",
      "learn\n",
      "drizzling\n",
      "beautiful\n",
      "kenny\n",
      "suggestions\n",
      "states\n",
      "something\n",
      "sense\n",
      "honest\n",
      "swedish\n",
      "huge\n",
      "needs\n",
      "agendas\n",
      "watching\n",
      "how\n",
      "pizza\n",
      "okay\n",
      "may\n",
      "after\n",
      "mac\n",
      "lay\n",
      "curiosity\n",
      "plann\n",
      "such\n",
      "guys\n",
      "cuddling\n",
      "tall\n",
      "wine\n",
      "refund\n",
      "deck\n",
      "its\n",
      "fit\n",
      "personal\n",
      "willing\n",
      "hidden\n",
      "soccer\n",
      "might\n",
      "then\n",
      "them\n",
      "return\n",
      "x__o\n",
      "food\n",
      "flour\n",
      "workout\n",
      "safe\n",
      "dat\n",
      "lunch\n",
      "bollywood\n",
      "possess\n",
      "assess\n",
      "name\n",
      "financial\n",
      "sexually\n",
      "thick\n",
      "inquires\n",
      "blackk\n",
      "little\n",
      "quite\n",
      "worries\n",
      "california\n",
      "ask\n",
      "partner\n",
      "longterm\n",
      "days\n",
      "interact\n",
      "south\n",
      "platonic\n",
      "feel\n",
      "number\n",
      "fancy\n",
      "date\n",
      "done\n",
      "lol\n",
      "communicator\n",
      "ring\n",
      "adventitious\n",
      "fire\n",
      "ptin\n",
      "their\n",
      "snuggling\n",
      "top\n",
      "night\n",
      "anyone\n",
      "ctec\n",
      "returns\n",
      "white\n",
      "relationship\n",
      "part\n",
      "somewhat\n",
      "std\n",
      "favors\n",
      "bed\n",
      "classes\n",
      "marriage\n",
      "venture\n",
      "lover\n",
      "listen\n",
      "tonight\n",
      "talking\n",
      "superman\n",
      "hispanic\n",
      "males\n",
      "chest\n",
      "which\n",
      "online\n",
      "sunnyvale\n",
      "though\n",
      "most\n",
      "mouth\n",
      "addict\n",
      "exercises\n",
      "audio\n",
      "sometimes\n",
      "particularly\n",
      "text\n",
      "agreed\n",
      "cheap\n",
      "bedroom\n",
      "find\n",
      "one\n",
      "busy\n",
      "title\n",
      "outside\n",
      "unhappy\n",
      "should\n",
      "athletic\n",
      "pretty\n",
      "circle\n",
      "local\n",
      "personality\n",
      "regularly\n",
      "hiv\n",
      "feels\n",
      "breast\n",
      "reply\n",
      "ema\n",
      "pimp\n",
      "artist\n",
      "shy\n",
      "married\n",
      "taxes\n",
      "bay\n",
      "stuff\n",
      "where\n",
      "husband\n",
      "ears\n",
      "college\n",
      "arm\n",
      "best\n",
      "arou\n",
      "visiting\n",
      "away\n",
      "currently\n",
      "calderone\n",
      "across\n",
      "latin\n",
      "missing\n",
      "however\n",
      "coffee\n",
      "last\n",
      "curly\n",
      "career\n",
      "joke\n",
      "taking\n",
      "connection\n",
      "alot\n",
      "simple\n",
      "laugh\n",
      "scottish\n",
      "5three\n",
      "trust\n",
      "comical\n",
      "deep\n",
      "fri\n",
      "treat\n",
      "meeting\n",
      "wants\n",
      "else\n",
      "understand\n",
      "lonely\n",
      "gunpowder\n",
      "three4seven\n",
      "these\n",
      "straight\n",
      "value\n",
      "choices\n",
      "while\n",
      "leaning\n",
      "seven\n",
      "engaged\n",
      "sustenance\n",
      "upscale\n",
      "piercings\n",
      "ready\n",
      "motivated\n",
      "perhaps\n",
      "make\n",
      "fling\n",
      "gentlemen\n",
      "largest\n",
      "party\n",
      "entitled\n",
      "week\n",
      "unconventional\n",
      "emotionally\n",
      "hang\n",
      "driven\n",
      "student\n",
      "expand\n",
      "lower\n",
      "practiced\n",
      "without\n",
      "qualities\n",
      "muscle\n",
      "left\n",
      "snowman\n",
      "outdoor\n",
      "being\n",
      "actress\n",
      "couch\n",
      "happiness\n",
      "cup\n",
      "except\n",
      "reasons\n",
      "lets\n",
      "easy\n",
      "crack\n",
      "tax\n",
      "has\n",
      "ski\n",
      "smart\n",
      "around\n",
      "big\n",
      "dark\n",
      "possibly\n",
      "lady\n",
      "apart\n",
      "intel\n",
      "specific\n",
      "overrated\n",
      "hung\n",
      "works\n",
      "deal\n",
      "hair\n",
      "demeanor\n",
      "proper\n",
      "curious\n",
      "humor\n",
      "legal\n",
      "creative\n",
      "does\n",
      "planni\n",
      "speak\n",
      "snuggle\n",
      "holiday\n",
      "patient\n",
      "business\n",
      "gentleman\n",
      "six0\n",
      "burner\n",
      "exciting\n",
      "working\n",
      "anything\n",
      "discussed\n",
      "drama\n",
      "sadistic\n",
      "whats\n",
      "year\n",
      "mba\n",
      "weather\n",
      "parties\n",
      "drifted\n",
      "strictly\n",
      "hey\n",
      "tame\n",
      "start\n",
      "accordingly\n",
      "way\n",
      "happy\n",
      "lowest\n",
      "tight\n",
      "offer\n",
      "searching\n",
      "deductions\n",
      "goals\n",
      "line\n",
      "attached\n",
      "flirt\n",
      "mature\n",
      "site\n",
      "unusually\n",
      "wicked\n",
      "similar\n",
      "7five3\n",
      "certain\n",
      "single\n",
      "right\n",
      "chat\n",
      "home\n",
      "holla\n",
      "scribe\n",
      "actor\n",
      "field\n",
      "details\n",
      "income\n",
      "picture\n",
      "scrub\n",
      "stay\n",
      "song\n",
      "longer\n",
      "phone\n",
      "dvd\n",
      "curvy\n",
      "back\n",
      "far\n",
      "serious\n",
      "songs\n"
     ]
    }
   ],
   "source": [
    "from python_week05 import bayes\n",
    "from python_week05 import feedparser\n",
    "import numpy\n",
    "\n",
    "### 1. 수집: RSS 피드 불러오기\n",
    "# # import feedparser\n",
    "# ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "# ny['entries']\n",
    "# len(ny['entries'])\n",
    "\n",
    "def calcMostFreq(vocabList,fullText):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token]=fullText.count(token)\n",
    "    sortedFreq = sorted(freqDict.iteritems(), key=operator.itemgetter(1), reverse=True) \n",
    "    return sortedFreq[:30]       \n",
    "\n",
    "def localWords(feed1,feed0):\n",
    "    #import feedparser\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1) #NY is class 1\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = bayes.createVocabList(docList)#create vocabulary\n",
    "    top30Words = calcMostFreq(vocabList,fullText)   #remove top 30 words\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    trainingSet = range(2*minLen); testSet=[]           #create test set\n",
    "    for i in range(20):\n",
    "        randIndex = int(numpy.random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bayes.bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = bayes.trainNB0(numpy.array(trainMat),numpy.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bayes.bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if bayes.classifyNB(numpy.array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print 'the error rate is: ',float(errorCount)/len(testSet)\n",
    "    return vocabList,p0V,p1V\n",
    "\n",
    "\n",
    "\n",
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "\n",
    "vocabList, pSF, pNY = localWords(ny, sf)\n",
    "\n",
    "\n",
    "### 2. 분석: 지역적으로 사용되는 단어 표현하기\n",
    "def getTopWords(ny,sf):\n",
    "    import operator\n",
    "    vocabList,p0V,p1V=localWords(ny,sf)\n",
    "    topNY=[]; topSF=[]\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0 : topSF.append((vocabList[i],p0V[i]))\n",
    "        if p1V[i] > -6.0 : topNY.append((vocabList[i],p1V[i]))\n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    print \"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\"\n",
    "    for item in sortedSF:\n",
    "        print item[0]\n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print \"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\"\n",
    "    for item in sortedNY:\n",
    "        print item[0]\n",
    "        \n",
    "getTopWords(ny, sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 예제: 문장을 긍정/부정으로 분류하기\n",
    "\n",
    "참고:http://stevenloria.com/how-to-build-a-text-classification-system-with-python-and-textblob/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beer was amazing.\n",
      "pos\n",
      "But the hangover was horrible.\n",
      "neg\n",
      "My boss was not pleased.\n",
      "neg\n",
      "Accuracy: 0.833333333333\n",
      "Most Informative Features\n",
      "          contains(this) = True              neg : pos    =      2.3 : 1.0\n",
      "          contains(this) = False             pos : neg    =      1.8 : 1.0\n",
      "          contains(This) = False             neg : pos    =      1.6 : 1.0\n",
      "            contains(an) = False             neg : pos    =      1.6 : 1.0\n",
      "             contains(I) = False             pos : neg    =      1.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob import TextBlob\n",
    "\n",
    "train = [\n",
    "    ('I love this sandwich.', 'pos'),\n",
    "    ('This is an amazing place!', 'pos'),\n",
    "    ('I feel very good about these beers.', 'pos'),\n",
    "    ('This is my best work.', 'pos'),\n",
    "    (\"What an awesome view\", 'pos'),\n",
    "    ('I do not like this restaurant', 'neg'),\n",
    "    ('I am tired of this stuff.', 'neg'),\n",
    "    (\"I can't deal with this\", 'neg'),\n",
    "    ('He is my sworn enemy!', 'neg'),\n",
    "    ('My boss is horrible.', 'neg')\n",
    "]\n",
    "test = [\n",
    "    ('The beer was good.', 'pos'),\n",
    "    ('I do not enjoy my job', 'neg'),\n",
    "    (\"I ain't feeling dandy today.\", 'neg'),\n",
    "    (\"I feel amazing!\", 'pos'),\n",
    "    ('Gary is a friend of mine.', 'pos'),\n",
    "    (\"I can't believe I'm doing this.\", 'neg')\n",
    "]\n",
    "\n",
    "cl = NaiveBayesClassifier(train)\n",
    "\n",
    "# Classify some text\n",
    "# print(cl.classify(\"Their burgers are amazing.\"))  # \"pos\"\n",
    "# print(cl.classify(\"I don't like their pizza.\"))   # \"neg\"\n",
    "\n",
    "# Classify a TextBlob\n",
    "blob = TextBlob(\"The beer was amazing. But the hangover was horrible. \"\n",
    "                \"My boss was not pleased.\", classifier=cl)\n",
    "# print(blob)\n",
    "# print(blob.classify())\n",
    "\n",
    "for sentence in blob.sentences:\n",
    "    print(sentence)\n",
    "    print(sentence.classify())\n",
    "\n",
    "# Compute accuracy\n",
    "print(\"Accuracy: {0}\".format(cl.accuracy(test)))\n",
    "\n",
    "# Show 5 most informative features\n",
    "cl.show_informative_features(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
