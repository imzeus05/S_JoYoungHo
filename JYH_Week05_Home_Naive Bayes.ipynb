{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 나이브 베이스\n",
    "\n",
    "### 나이브 베이스에 대한 접근 방법\n",
    "1. 수집 : RSS 자료\n",
    "2. 준비 : 명목형, 부울형\n",
    "3. 분석 : 히스토그램으로 분석\n",
    "4. 훈련 : 각 속성을 독립적으로 조건부 확률 계산\n",
    "5. 검사 : 오류율 검사\n",
    "6. 사용 : 문서 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이썬으로 텍스트 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "### 1. 준비 : 텍스트로 단어 벡터 만들기\n",
    "\n",
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec\n",
    "                 \n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print \"the word: %s is not in my Vocabulary!\" % word\n",
    "    return returnVec\n",
    "\n",
    "\n",
    "\n",
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "print myVocabList\n",
    "\n",
    "print setOfWords2Vec(myVocabList, listOPosts[0])\n",
    "print setOfWords2Vec(myVocabList, listOPosts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.5\n",
      "[ 0.04166667  0.04166667  0.04166667  0.          0.          0.04166667\n",
      "  0.04166667  0.04166667  0.          0.04166667  0.04166667  0.04166667\n",
      "  0.04166667  0.          0.          0.08333333  0.          0.\n",
      "  0.04166667  0.          0.04166667  0.04166667  0.          0.04166667\n",
      "  0.04166667  0.04166667  0.          0.04166667  0.          0.04166667\n",
      "  0.04166667  0.125     ]\n",
      "[ 0.          0.          0.          0.05263158  0.05263158  0.          0.\n",
      "  0.          0.05263158  0.05263158  0.          0.          0.\n",
      "  0.05263158  0.05263158  0.05263158  0.05263158  0.05263158  0.\n",
      "  0.10526316  0.          0.05263158  0.05263158  0.          0.10526316\n",
      "  0.          0.15789474  0.          0.05263158  0.          0.          0.        ]\n",
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "### 2. 훈련: 단어 벡터로 확률 계산하기\n",
    "from numpy import *\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = zeros(numWords); p1Num = zeros(numWords)\n",
    "    p0Denom = 0.0; p1Denom = 0.0                    \n",
    "#     p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() \n",
    "#     p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "#     p1Vect = log(p1Num/p1Denom)          #change to log()\n",
    "#     p0Vect = log(p0Num/p0Denom)          #change to log()\n",
    "    p1Vect = p1Num/p1Denom          #change to log()\n",
    "    p0Vect = p0Num/p0Denom          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive\n",
    "\n",
    "\n",
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "trainMat=[]\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    \n",
    "    \n",
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "\n",
    "print pAb\n",
    "print p0V\n",
    "print p1V\n",
    "# 책과 p0V, p1V 결과가 다름\n",
    "\n",
    "### 3. 검사: 실제 조건을 반영하기 위해 분류기 수정하기\n",
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "    \n",
    "testingNB()\n",
    "\n",
    "\n",
    "### 4. 준비: 중복 단어 문서 모델\n",
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제: 스팸 이메일 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M.L.', 'I', 'have', 'ever', 'laid', 'eyes', 'upon']\n",
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon']\n",
      "['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'm', 'l', 'i', 'have', 'ever', 'laid', 'eyes', 'upon']\n",
      "['Hello', 'Since', 'you', 'are', 'an', 'owner', 'of', 'at', 'least', 'one', 'Google', 'Groups', 'group', 'that', 'uses', 'the', 'customized', 'welcome', 'message', 'pages', 'or', 'files', 'we', 'are', 'writing', 'to', 'inform', 'you', 'that', 'we', 'will', 'no', 'longer', 'be', 'supporting', 'these', 'features', 'starting', 'February', '2011', 'We', 'made', 'this', 'decision', 'so', 'that', 'we', 'can', 'focus', 'on', 'improving', 'the', 'core', 'functionalities', 'of', 'Google', 'Groups', 'mailing', 'lists', 'and', 'forum', 'discussions', 'Instead', 'of', 'these', 'features', 'we', 'encourage', 'you', 'to', 'use', 'products', 'that', 'are', 'designed', 'specifically', 'for', 'file', 'storage', 'and', 'page', 'creation', 'such', 'as', 'Google', 'Docs', 'and', 'Google', 'Sites', 'For', 'example', 'you', 'can', 'easily', 'create', 'your', 'pages', 'on', 'Google', 'Sites', 'and', 'share', 'the', 'site', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '174623', 'with', 'the', 'members', 'of', 'your', 'group', 'You', 'can', 'also', 'store', 'your', 'files', 'on', 'the', 'site', 'by', 'attaching', 'files', 'to', 'pages', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '90563', 'on', 'the', 'site', 'If', 'you', 're', 'just', 'looking', 'for', 'a', 'place', 'to', 'upload', 'your', 'files', 'so', 'that', 'your', 'group', 'members', 'can', 'download', 'them', 'we', 'suggest', 'you', 'try', 'Google', 'Docs', 'You', 'can', 'upload', 'files', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '50092', 'and', 'share', 'access', 'with', 'either', 'a', 'group', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '66343', 'or', 'an', 'individual', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '86152', 'assigning', 'either', 'edit', 'or', 'download', 'only', 'access', 'to', 'the', 'files', 'you', 'have', 'received', 'this', 'mandatory', 'email', 'service', 'announcement', 'to', 'update', 'you', 'about', 'important', 'changes', 'to', 'Google', 'Groups', '']\n",
      "classification error ['home', 'based', 'business', 'opportunity', 'knocking', 'your', 'door', 'don', 'rude', 'and', 'let', 'this', 'chance', 'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts']\n",
      "the error rate is:  0.1\n",
      "classification error ['yay', 'you', 'both', 'doing', 'fine', 'working', 'mba', 'design', 'strategy', 'cca', 'top', 'art', 'school', 'new', 'program', 'focusing', 'more', 'right', 'brained', 'creative', 'and', 'strategic', 'approach', 'management', 'the', 'way', 'done', 'today']\n",
      "the error rate is:  0.1\n",
      "the error rate is:  0.0\n"
     ]
    }
   ],
   "source": [
    "#import python_week05.bayes\n",
    "from python_week05 import bayes\n",
    "import numpy\n",
    "\n",
    "### 1. 준비: 텍스트 토큰 만들기\n",
    "mySent='This book is the best book on Python or M.L. I have ever laid eyes upon'\n",
    "print mySent.split()\n",
    "\n",
    "import re\n",
    "regEx = re.compile('\\\\W*')\n",
    "listOfTokens = regEx.split(mySent)\n",
    "print listOfTokens\n",
    "# 책과 결과가 다름 (공백 없는 데이터가 없음)\n",
    "\n",
    "print [tok.lower() for tok in listOfTokens if len(tok) > 0]\n",
    "\n",
    "emailText = open('python_week05/email/ham/6.txt').read()\n",
    "listOfTokens = regEx.split(emailText)\n",
    "print listOfTokens\n",
    "\n",
    "### 2. 검사: 나이브 베이스로 교차 검증하기\n",
    "def textParse(bigString):    #input is big string, #output is word list\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] \n",
    "    \n",
    "def spamTest():\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open('python_week05/email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('python_week05/email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = bayes.createVocabList(docList)#create vocabulary\n",
    "    trainingSet = range(50); testSet=[]           #create test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(numpy.random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bayes.bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = bayes.trainNB0(numpy.array(trainMat),numpy.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bayes.bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if bayes.classifyNB(numpy.array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print \"classification error\",docList[docIndex]\n",
    "    print 'the error rate is: ',float(errorCount)/len(testSet)\n",
    "    #return vocabList,fullText\n",
    "    \n",
    "\n",
    "spamTest()\n",
    "spamTest()\n",
    "spamTest()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 예제: 개인 광고에 포함된 지역 특색 도출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.25\n",
      "the error rate is:  0.4\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "she\n",
      "great\n",
      "talk\n",
      "asian\n",
      "fan\n",
      "maybe\n",
      "eyes\n",
      "too\n",
      "going\n",
      "older\n",
      "know\n",
      "about\n",
      "interested\n",
      "friendly\n",
      "what\n",
      "relaxation\n",
      "healthy\n",
      "smoke\n",
      "partner\n",
      "then\n",
      "half\n",
      "year\n",
      "our\n",
      "dancing\n",
      "hazel\n",
      "don\n",
      "get\n",
      "sex\n",
      "best\n",
      "please\n",
      "church\n",
      "chill\n",
      "straight\n",
      "make\n",
      "any\n",
      "shape\n",
      "day\n",
      "lets\n",
      "old\n",
      "back\n",
      "420\n",
      "burning\n",
      "bud\n",
      "made\n",
      "comics\n",
      "chat\n",
      "really\n",
      "hello\n",
      "all\n",
      "month\n",
      "integrity\n",
      "children\n",
      "young\n",
      "languages\n",
      "stable\n",
      "drinking\n",
      "try\n",
      "small\n",
      "round\n",
      "spiritual\n",
      "cyber\n",
      "past\n",
      "traveling\n",
      "pedi\n",
      "click\n",
      "reply\n",
      "nurse\n",
      "public\n",
      "seeks\n",
      "body\n",
      "christian\n",
      "getaways\n",
      "here\n",
      "let\n",
      "active\n",
      "105\n",
      "experience\n",
      "accomplished\n",
      "chaste\n",
      "killer\n",
      "love\n",
      "secure\n",
      "family\n",
      "festivals\n",
      "sports\n",
      "live\n",
      "calm\n",
      "loving\n",
      "musician\n",
      "train\n",
      "hold\n",
      "must\n",
      "town\n",
      "work\n",
      "cuddle\n",
      "brown\n",
      "favorite\n",
      "slim\n",
      "high\n",
      "huge\n",
      "travel\n",
      "intelligent\n",
      "fat\n",
      "such\n",
      "short\n",
      "physical\n",
      "neck\n",
      "form\n",
      "putting\n",
      "wine\n",
      "over\n",
      "soon\n",
      "years\n",
      "scientist\n",
      "fit\n",
      "till\n",
      "highlights\n",
      "attraction\n",
      "willing\n",
      "degrees\n",
      "2016\n",
      "somebody\n",
      "food\n",
      "therapy\n",
      "kiss\n",
      "discuss\n",
      "name\n",
      "rock\n",
      "friendship\n",
      "financial\n",
      "bay\n",
      "sexual\n",
      "laid\n",
      "red\n",
      "shows\n",
      "little\n",
      "free\n",
      "9er\n",
      "tasting\n",
      "length\n",
      "place\n",
      "feel\n",
      "thick\n",
      "district\n",
      "girls\n",
      "smile\n",
      "white\n",
      "friend\n",
      "hangout\n",
      "gym\n",
      "butt\n",
      "yours\n",
      "kind\n",
      "yrs\n",
      "mind\n",
      "sometimes\n",
      "hug\n",
      "breakfast\n",
      "tells\n",
      "warriors\n",
      "answering\n",
      "without\n",
      "latina\n",
      "plenty\n",
      "michelle\n",
      "metal\n",
      "face\n",
      "looked\n",
      "clean\n",
      "weigh\n",
      "shop\n",
      "text\n",
      "random\n",
      "fine\n",
      "enthusiast\n",
      "justice\n",
      "sugar\n",
      "hope\n",
      "giants\n",
      "married\n",
      "husband\n",
      "sho\n",
      "set\n",
      "culture\n",
      "see\n",
      "suttle\n",
      "movie\n",
      "yet\n",
      "encounters\n",
      "disgruntled\n",
      "never\n",
      "coffee\n",
      "both\n",
      "daddy\n",
      "last\n",
      "career\n",
      "monk\n",
      "mani\n",
      "point\n",
      "alone\n",
      "sammy\n",
      "speak\n",
      "tint\n",
      "blink\n",
      "treat\n",
      "women\n",
      "waste\n",
      "wants\n",
      "life\n",
      "kisses\n",
      "catch\n",
      "smoking\n",
      "myself\n",
      "will\n",
      "fun\n",
      "things\n",
      "same\n",
      "romance\n",
      "buddhism\n",
      "phd\n",
      "cartoon\n",
      "dark\n",
      "safe\n",
      "well\n",
      "seems\n",
      "other\n",
      "easy\n",
      "has\n",
      "around\n",
      "read\n",
      "big\n",
      "couple\n",
      "early\n",
      "lady\n",
      "shoulder\n",
      "spit\n",
      "nude\n",
      "hinduism\n",
      "become\n",
      "soft\n",
      "performing\n",
      "some\n",
      "hair\n",
      "jla\n",
      "home\n",
      "daytime\n",
      "does\n",
      "each\n",
      "pressure\n",
      "includes\n",
      "anything\n",
      "god\n",
      "drama\n",
      "seeing\n",
      "whats\n",
      "next\n",
      "area\n",
      "question\n",
      "lot\n",
      "batman\n",
      "hear\n",
      "faster\n",
      "cuddles\n",
      "hispanic\n",
      "single\n",
      "happen\n",
      "beyond\n",
      "thanks\n",
      "evenings\n",
      "teeth\n",
      "stay\n",
      "professional\n",
      "photos\n",
      "age\n",
      "having\n",
      "consider\n",
      "relieve\n",
      "results\n",
      "thoughtful\n",
      "hanging\n",
      "lace\n",
      "apartment\n",
      "ver\n",
      "veo\n",
      "enlightenment\n",
      "activities\n",
      "honest\n",
      "sitting\n",
      "very\n",
      "showcontact\n",
      "minded\n",
      "cool\n",
      "school\n",
      "doesn\n",
      "level\n",
      "did\n",
      "wednesday\n",
      "sane\n",
      "joke\n",
      "enjoy\n",
      "says\n",
      "likely\n",
      "further\n",
      "friendships\n",
      "drinks\n",
      "even\n",
      "revolves\n",
      "while\n",
      "current\n",
      "ever\n",
      "full\n",
      "newyork\n",
      "listening\n",
      "hours\n",
      "explore\n",
      "along\n",
      "strong\n",
      "engage\n",
      "leaves\n",
      "leaving\n",
      "prior\n",
      "usually\n",
      "suitable\n",
      "pillow\n",
      "massage\n",
      "thats\n",
      "randle79\n",
      "href\n",
      "expirence\n",
      "siempre\n",
      "from\n",
      "texting\n",
      "conoces\n",
      "few\n",
      "todas\n",
      "wondering\n",
      "type\n",
      "knows\n",
      "relax\n",
      "started\n",
      "share\n",
      "phone\n",
      "stick\n",
      "excellent\n",
      "join\n",
      "roosevelt\n",
      "movies\n",
      "thin\n",
      "male\n",
      "organized\n",
      "something\n",
      "want\n",
      "keep\n",
      "plent\n",
      "watching\n",
      "how\n",
      "hot\n",
      "nyc\n",
      "massive\n",
      "tension\n",
      "okay\n",
      "petite\n",
      "may\n",
      "fart\n",
      "date\n",
      "upstate\n",
      "cuddling\n",
      "5514380879\n",
      "office\n",
      "through\n",
      "still\n",
      "its\n",
      "perfect\n",
      "clo\n",
      "tboys\n",
      "thank\n",
      "mattress\n",
      "them\n",
      "ten14\n",
      "lunch\n",
      "now\n",
      "always\n",
      "drop\n",
      "found\n",
      "lasting\n",
      "everyone\n",
      "gaming\n",
      "doing\n",
      "house\n",
      "idea\n",
      "oil\n",
      "connect\n",
      "girl\n",
      "special\n",
      "living\n",
      "alcohol\n",
      "open\n",
      "since\n",
      "forty\n",
      "got\n",
      "caucasian\n",
      "assured\n",
      "thong\n",
      "put\n",
      "org\n",
      "gusta\n",
      "could\n",
      "times\n",
      "conversation\n",
      "isn\n",
      "511\n",
      "think\n",
      "mad\n",
      "dont\n",
      "shemales\n",
      "one\n",
      "introspective\n",
      "done\n",
      "spiritually\n",
      "another\n",
      "sober\n",
      "chubby\n",
      "miss\n",
      "righ\n",
      "start\n",
      "twenty\n",
      "anyone\n",
      "twenties\n",
      "las\n",
      "listen\n",
      "introvert\n",
      "mostly\n",
      "exactly\n",
      "because\n",
      "copy\n",
      "loved\n",
      "hike\n",
      "bed\n",
      "matter\n",
      "decided\n",
      "stp\n",
      "feeling\n",
      "san\n",
      "comfortable\n",
      "tonight\n",
      "need\n",
      "mountain\n",
      "relationship\n",
      "pasan\n",
      "able\n",
      "mid\n",
      "oriented\n",
      "equipment\n",
      "pasar\n",
      "which\n",
      "openimnded\n",
      "panties\n",
      "graduating\n",
      "anythi\n",
      "most\n",
      "experiences\n",
      "tall\n",
      "nothing\n",
      "why\n",
      "queens\n",
      "dom\n",
      "average\n",
      "later\n",
      "dog\n",
      "judgmental\n",
      "senior\n",
      "stoner\n",
      "walking\n",
      "show\n",
      "meetings\n",
      "bring\n",
      "aggressive\n",
      "find\n",
      "giant\n",
      "busy\n",
      "joyeria\n",
      "title\n",
      "only\n",
      "matters\n",
      "9one7\n",
      "intellectually\n",
      "watch\n",
      "photograph\n",
      "truly\n",
      "hall\n",
      "consist\n",
      "settings\n",
      "morning\n",
      "bad\n",
      "boring\n",
      "where\n",
      "individual\n",
      "college\n",
      "close\n",
      "submissives\n",
      "fifties\n",
      "between\n",
      "probably\n",
      "email\n",
      "skimpy\n",
      "recently\n",
      "wears\n",
      "attention\n",
      "however\n",
      "job\n",
      "key\n",
      "optional\n",
      "transex\n",
      "many\n",
      "connection\n",
      "interesting\n",
      "color\n",
      "trust\n",
      "mission\n",
      "caring\n",
      "companionship\n",
      "chick\n",
      "monthly\n",
      "come\n",
      "been\n",
      "whom\n",
      "beer\n",
      "much\n",
      "chatting\n",
      "sisterhood\n",
      "attractive\n",
      "educated\n",
      "families\n",
      "relaxing\n",
      "drugs\n",
      "else\n",
      "acti\n",
      "kids\n",
      "muscular\n",
      "aspiring\n",
      "entrepreneurial\n",
      "misses\n",
      "kissing\n",
      "biking\n",
      "two14\n",
      "hoping\n",
      "canoeing\n",
      "almost\n",
      "sincere\n",
      "non\n",
      "sleeping\n",
      "ave\n",
      "seattle\n",
      "amount\n",
      "craigslist\n",
      "exhibitionists\n",
      "gets\n",
      "difficult\n",
      "practice\n",
      "http\n",
      "drink\n",
      "hang\n",
      "kik\n",
      "meeti\n",
      "totally\n",
      "expand\n",
      "built\n",
      "off\n",
      "youre\n",
      "morrow\n",
      "person\n",
      "contact\n",
      "spend\n",
      "outdoor\n",
      "they\n",
      "being\n",
      "front\n",
      "openly\n",
      "fishing\n",
      "regardless\n",
      "yes\n",
      "graduate\n",
      "satin\n",
      "bit\n",
      "jose\n",
      "cut\n",
      "ads\n",
      "grounded\n",
      "also\n",
      "cuz\n",
      "eager\n",
      "board\n",
      "administrative\n",
      "build\n",
      "real\n",
      "possible\n",
      "possibly\n",
      "five\n",
      "mom\n",
      "wanting\n",
      "boyshort\n",
      "using\n",
      "gustan\n",
      "unique\n",
      "hunt\n",
      "benefit\n",
      "night\n",
      "hung\n",
      "passionate\n",
      "seating\n",
      "people\n",
      "spring\n",
      "pals\n",
      "lotions\n",
      "soothing\n",
      "witty\n",
      "either\n",
      "core\n",
      "business\n",
      "intimate\n",
      "rub\n",
      "marketing\n",
      "although\n",
      "muchas\n",
      "post\n",
      "stage\n",
      "lifestyle\n",
      "getting\n",
      "filipina\n",
      "industry\n",
      "discussed\n",
      "location\n",
      "decent\n",
      "sadistic\n",
      "road\n",
      "ladies\n",
      "own\n",
      "visit\n",
      "errands\n",
      "snuggling\n",
      "down\n",
      "doesnt\n",
      "involvement\n",
      "female\n",
      "actually\n",
      "her\n",
      "there\n",
      "hey\n",
      "long\n",
      "class\n",
      "ganja\n",
      "occupy\n",
      "lonely\n",
      "low\n",
      "forward\n",
      "seasoned\n",
      "bored\n",
      "was\n",
      "offering\n",
      "eag\n",
      "nassau\n",
      "line\n",
      "info\n",
      "places\n",
      "meeting\n",
      "attached\n",
      "mature\n",
      "similar\n",
      "detailed\n",
      "pic\n",
      "moved\n",
      "deep\n",
      "hardly\n",
      "sometime\n",
      "right\n",
      "daily\n",
      "curious\n",
      "annoying\n",
      "film\n",
      "posting\n",
      "gay\n",
      "illinois\n",
      "successful\n",
      "details\n",
      "nice\n",
      "energetic\n",
      "accidental\n",
      "june\n",
      "helping\n",
      "recent\n",
      "peak\n",
      "song\n",
      "goin\n",
      "younger\n",
      "lead\n",
      "greetings\n",
      "together\n",
      "hikes\n",
      "serious\n",
      "songs\n",
      "hiking\n",
      "basically\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n",
      "male\n",
      "there\n",
      "single\n",
      "together\n",
      "something\n",
      "one\n",
      "white\n",
      "clean\n",
      "life\n",
      "around\n",
      "honest\n",
      "very\n",
      "did\n",
      "what\n",
      "few\n",
      "years\n",
      "through\n",
      "now\n",
      "little\n",
      "conversation\n",
      "twenties\n",
      "comfortable\n",
      "need\n",
      "relationship\n",
      "most\n",
      "don\n",
      "show\n",
      "title\n",
      "get\n",
      "watch\n",
      "email\n",
      "attractive\n",
      "kids\n",
      "things\n",
      "contact\n",
      "being\n",
      "wanting\n",
      "marketing\n",
      "next\n",
      "info\n",
      "pic\n",
      "chat\n",
      "illinois\n",
      "nice\n",
      "professional\n",
      "all\n",
      "consider\n",
      "asian\n",
      "hanging\n",
      "lace\n",
      "apartment\n",
      "ver\n",
      "young\n",
      "veo\n",
      "sitting\n",
      "showcontact\n",
      "minded\n",
      "school\n",
      "doesn\n",
      "level\n",
      "small\n",
      "says\n",
      "past\n",
      "likely\n",
      "click\n",
      "even\n",
      "revolves\n",
      "current\n",
      "reply\n",
      "ever\n",
      "full\n",
      "newyork\n",
      "here\n",
      "let\n",
      "active\n",
      "along\n",
      "leaves\n",
      "leaving\n",
      "prior\n",
      "smoke\n",
      "randle79\n",
      "href\n",
      "expirence\n",
      "siempre\n",
      "texting\n",
      "conoces\n",
      "todas\n",
      "type\n",
      "share\n",
      "phone\n",
      "stick\n",
      "excellent\n",
      "hold\n",
      "work\n",
      "roosevelt\n",
      "cuddle\n",
      "thin\n",
      "slim\n",
      "organized\n",
      "want\n",
      "keep\n",
      "watching\n",
      "nyc\n",
      "okay\n",
      "may\n",
      "cuddling\n",
      "5514380879\n",
      "office\n",
      "over\n",
      "still\n",
      "perfect\n",
      "tboys\n",
      "always\n",
      "drop\n",
      "friendship\n",
      "everyone\n",
      "doing\n",
      "house\n",
      "idea\n",
      "connect\n",
      "year\n",
      "girl\n",
      "living\n",
      "alcohol\n",
      "forty\n",
      "caucasian\n",
      "assured\n",
      "thong\n",
      "put\n",
      "org\n",
      "gusta\n",
      "could\n",
      "isn\n",
      "511\n",
      "think\n",
      "shemales\n",
      "done\n",
      "another\n",
      "sober\n",
      "chubby\n",
      "miss\n",
      "righ\n",
      "start\n",
      "twenty\n",
      "anyone\n",
      "las\n",
      "friend\n",
      "introvert\n",
      "exactly\n",
      "copy\n",
      "kind\n",
      "bed\n",
      "matter\n",
      "stp\n",
      "feeling\n",
      "mind\n",
      "sometimes\n",
      "pasan\n",
      "able\n",
      "mid\n",
      "oriented\n",
      "equipment\n",
      "pasar\n",
      "latina\n",
      "openimnded\n",
      "panties\n",
      "graduating\n",
      "anythi\n",
      "experiences\n",
      "queens\n",
      "senior\n",
      "stoner\n",
      "text\n",
      "find\n",
      "busy\n",
      "joyeria\n",
      "only\n",
      "going\n",
      "matters\n",
      "photograph\n",
      "truly\n",
      "settings\n",
      "morning\n",
      "bad\n",
      "see\n",
      "individual\n",
      "college\n",
      "submissives\n",
      "please\n",
      "fifties\n",
      "probably\n",
      "skimpy\n",
      "job\n",
      "optional\n",
      "transex\n",
      "career\n",
      "many\n",
      "interesting\n",
      "caring\n",
      "companionship\n",
      "been\n",
      "whom\n",
      "chatting\n",
      "treat\n",
      "wants\n",
      "educated\n",
      "families\n",
      "drugs\n",
      "muscular\n",
      "myself\n",
      "aspiring\n",
      "will\n",
      "entrepreneurial\n",
      "fun\n",
      "kissing\n",
      "hoping\n",
      "almost\n",
      "sincere\n",
      "ave\n",
      "seattle\n",
      "make\n",
      "craigslist\n",
      "exhibitionists\n",
      "difficult\n",
      "http\n",
      "drink\n",
      "kik\n",
      "totally\n",
      "well\n",
      "spend\n",
      "they\n",
      "front\n",
      "shape\n",
      "yes\n",
      "graduate\n",
      "satin\n",
      "bit\n",
      "cut\n",
      "ads\n",
      "grounded\n",
      "board\n",
      "easy\n",
      "has\n",
      "administrative\n",
      "build\n",
      "real\n",
      "big\n",
      "couple\n",
      "five\n",
      "boyshort\n",
      "using\n",
      "gustan\n",
      "unique\n",
      "night\n",
      "old\n",
      "spring\n",
      "witty\n",
      "either\n",
      "business\n",
      "although\n",
      "muchas\n",
      "stage\n",
      "about\n",
      "lifestyle\n",
      "getting\n",
      "industry\n",
      "location\n",
      "decent\n",
      "own\n",
      "errands\n",
      "snuggling\n",
      "involvement\n",
      "female\n",
      "her\n",
      "hey\n",
      "class\n",
      "lonely\n",
      "seasoned\n",
      "was\n",
      "eag\n",
      "nassau\n",
      "line\n",
      "meeting\n",
      "detailed\n",
      "moved\n",
      "hardly\n",
      "right\n",
      "curious\n",
      "film\n",
      "successful\n",
      "energetic\n",
      "june\n",
      "recent\n",
      "song\n",
      "younger\n",
      "age\n",
      "greetings\n",
      "relieve\n",
      "results\n",
      "month\n",
      "thoughtful\n",
      "integrity\n",
      "children\n",
      "languages\n",
      "stable\n",
      "enlightenment\n",
      "friendly\n",
      "activities\n",
      "fan\n",
      "drinking\n",
      "cool\n",
      "wednesday\n",
      "try\n",
      "sane\n",
      "joke\n",
      "round\n",
      "enjoy\n",
      "spiritual\n",
      "cyber\n",
      "traveling\n",
      "further\n",
      "friendships\n",
      "pedi\n",
      "drinks\n",
      "while\n",
      "nurse\n",
      "public\n",
      "seeks\n",
      "body\n",
      "christian\n",
      "getaways\n",
      "listening\n",
      "hours\n",
      "explore\n",
      "relaxation\n",
      "strong\n",
      "105\n",
      "great\n",
      "engage\n",
      "healthy\n",
      "experience\n",
      "accomplished\n",
      "usually\n",
      "suitable\n",
      "pillow\n",
      "massage\n",
      "thats\n",
      "chaste\n",
      "killer\n",
      "love\n",
      "secure\n",
      "family\n",
      "festivals\n",
      "from\n",
      "sports\n",
      "live\n",
      "calm\n",
      "wondering\n",
      "loving\n",
      "knows\n",
      "relax\n",
      "started\n",
      "musician\n",
      "train\n",
      "must\n",
      "town\n",
      "join\n",
      "movies\n",
      "brown\n",
      "favorite\n",
      "high\n",
      "partner\n",
      "huge\n",
      "plent\n",
      "travel\n",
      "how\n",
      "hot\n",
      "intelligent\n",
      "massive\n",
      "tension\n",
      "petite\n",
      "fart\n",
      "fat\n",
      "date\n",
      "such\n",
      "upstate\n",
      "short\n",
      "physical\n",
      "neck\n",
      "maybe\n",
      "form\n",
      "putting\n",
      "talk\n",
      "wine\n",
      "soon\n",
      "scientist\n",
      "its\n",
      "clo\n",
      "thank\n",
      "fit\n",
      "till\n",
      "highlights\n",
      "mattress\n",
      "attraction\n",
      "willing\n",
      "degrees\n",
      "2016\n",
      "then\n",
      "them\n",
      "somebody\n",
      "food\n",
      "ten14\n",
      "lunch\n",
      "therapy\n",
      "kiss\n",
      "half\n",
      "discuss\n",
      "name\n",
      "rock\n",
      "found\n",
      "lasting\n",
      "financial\n",
      "gaming\n",
      "oil\n",
      "our\n",
      "bay\n",
      "sexual\n",
      "special\n",
      "open\n",
      "since\n",
      "laid\n",
      "got\n",
      "red\n",
      "shows\n",
      "free\n",
      "9er\n",
      "tasting\n",
      "times\n",
      "length\n",
      "place\n",
      "mad\n",
      "dont\n",
      "feel\n",
      "introspective\n",
      "spiritually\n",
      "thick\n",
      "district\n",
      "girls\n",
      "smile\n",
      "listen\n",
      "eyes\n",
      "hangout\n",
      "mostly\n",
      "gym\n",
      "because\n",
      "too\n",
      "butt\n",
      "yours\n",
      "loved\n",
      "yrs\n",
      "hike\n",
      "decided\n",
      "san\n",
      "tonight\n",
      "hug\n",
      "breakfast\n",
      "tells\n",
      "mountain\n",
      "dancing\n",
      "warriors\n",
      "answering\n",
      "without\n",
      "which\n",
      "hazel\n",
      "plenty\n",
      "tall\n",
      "nothing\n",
      "why\n",
      "michelle\n",
      "dom\n",
      "average\n",
      "later\n",
      "metal\n",
      "dog\n",
      "face\n",
      "looked\n",
      "judgmental\n",
      "weigh\n",
      "shop\n",
      "walking\n",
      "meetings\n",
      "random\n",
      "bring\n",
      "aggressive\n",
      "fine\n",
      "enthusiast\n",
      "giant\n",
      "justice\n",
      "sugar\n",
      "9one7\n",
      "hope\n",
      "giants\n",
      "intellectually\n",
      "hall\n",
      "consist\n",
      "married\n",
      "she\n",
      "boring\n",
      "where\n",
      "husband\n",
      "sho\n",
      "set\n",
      "sex\n",
      "culture\n",
      "close\n",
      "best\n",
      "suttle\n",
      "movie\n",
      "yet\n",
      "between\n",
      "encounters\n",
      "disgruntled\n",
      "never\n",
      "recently\n",
      "wears\n",
      "attention\n",
      "however\n",
      "coffee\n",
      "key\n",
      "both\n",
      "daddy\n",
      "last\n",
      "monk\n",
      "connection\n",
      "mani\n",
      "point\n",
      "color\n",
      "alone\n",
      "church\n",
      "sammy\n",
      "trust\n",
      "mission\n",
      "speak\n",
      "tint\n",
      "chick\n",
      "monthly\n",
      "come\n",
      "blink\n",
      "beer\n",
      "much\n",
      "women\n",
      "waste\n",
      "sisterhood\n",
      "relaxing\n",
      "kisses\n",
      "else\n",
      "acti\n",
      "catch\n",
      "smoking\n",
      "chill\n",
      "straight\n",
      "misses\n",
      "biking\n",
      "two14\n",
      "canoeing\n",
      "non\n",
      "sleeping\n",
      "amount\n",
      "same\n",
      "any\n",
      "romance\n",
      "buddhism\n",
      "gets\n",
      "practice\n",
      "hang\n",
      "phd\n",
      "meeti\n",
      "cartoon\n",
      "expand\n",
      "built\n",
      "dark\n",
      "safe\n",
      "off\n",
      "older\n",
      "youre\n",
      "morrow\n",
      "person\n",
      "outdoor\n",
      "openly\n",
      "fishing\n",
      "regardless\n",
      "jose\n",
      "also\n",
      "seems\n",
      "cuz\n",
      "day\n",
      "eager\n",
      "lets\n",
      "other\n",
      "read\n",
      "possible\n",
      "early\n",
      "possibly\n",
      "know\n",
      "mom\n",
      "lady\n",
      "shoulder\n",
      "spit\n",
      "nude\n",
      "hunt\n",
      "benefit\n",
      "hinduism\n",
      "hung\n",
      "become\n",
      "passionate\n",
      "soft\n",
      "seating\n",
      "performing\n",
      "people\n",
      "pals\n",
      "some\n",
      "back\n",
      "hair\n",
      "jla\n",
      "home\n",
      "lotions\n",
      "soothing\n",
      "daytime\n",
      "does\n",
      "420\n",
      "core\n",
      "intimate\n",
      "rub\n",
      "each\n",
      "pressure\n",
      "post\n",
      "includes\n",
      "anything\n",
      "filipina\n",
      "god\n",
      "discussed\n",
      "drama\n",
      "sadistic\n",
      "road\n",
      "seeing\n",
      "ladies\n",
      "whats\n",
      "burning\n",
      "visit\n",
      "down\n",
      "doesnt\n",
      "actually\n",
      "area\n",
      "question\n",
      "long\n",
      "ganja\n",
      "occupy\n",
      "low\n",
      "lot\n",
      "forward\n",
      "bored\n",
      "offering\n",
      "batman\n",
      "hear\n",
      "bud\n",
      "faster\n",
      "made\n",
      "places\n",
      "comics\n",
      "attached\n",
      "mature\n",
      "similar\n",
      "cuddles\n",
      "hispanic\n",
      "deep\n",
      "sometime\n",
      "daily\n",
      "annoying\n",
      "happen\n",
      "posting\n",
      "beyond\n",
      "gay\n",
      "thanks\n",
      "interested\n",
      "details\n",
      "evenings\n",
      "really\n",
      "accidental\n",
      "teeth\n",
      "stay\n",
      "helping\n",
      "peak\n",
      "goin\n",
      "lead\n",
      "photos\n",
      "having\n",
      "hikes\n",
      "serious\n",
      "songs\n",
      "hello\n",
      "hiking\n",
      "basically\n"
     ]
    }
   ],
   "source": [
    "from python_week05 import bayes\n",
    "from python_week05 import feedparser\n",
    "import numpy\n",
    "\n",
    "### 1. 수집: RSS 피드 불러오기\n",
    "# # import feedparser\n",
    "# ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "# ny['entries']\n",
    "# len(ny['entries'])\n",
    "\n",
    "def calcMostFreq(vocabList,fullText):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token]=fullText.count(token)\n",
    "    sortedFreq = sorted(freqDict.iteritems(), key=operator.itemgetter(1), reverse=True) \n",
    "    return sortedFreq[:30]       \n",
    "\n",
    "def localWords(feed1,feed0):\n",
    "    #import feedparser\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1) #NY is class 1\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = bayes.createVocabList(docList)#create vocabulary\n",
    "    top30Words = calcMostFreq(vocabList,fullText)   #remove top 30 words\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    trainingSet = range(2*minLen); testSet=[]           #create test set\n",
    "    for i in range(20):\n",
    "        randIndex = int(numpy.random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bayes.bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = bayes.trainNB0(numpy.array(trainMat),numpy.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bayes.bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if bayes.classifyNB(numpy.array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print 'the error rate is: ',float(errorCount)/len(testSet)\n",
    "    return vocabList,p0V,p1V\n",
    "\n",
    "\n",
    "\n",
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "\n",
    "vocabList, pSF, pNY = localWords(ny, sf)\n",
    "\n",
    "\n",
    "### 2. 분석: 지역적으로 사용되는 단어 표현하기\n",
    "def getTopWords(ny,sf):\n",
    "    import operator\n",
    "    vocabList,p0V,p1V=localWords(ny,sf)\n",
    "    topNY=[]; topSF=[]\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0 : topSF.append((vocabList[i],p0V[i]))\n",
    "        if p1V[i] > -6.0 : topNY.append((vocabList[i],p1V[i]))\n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n",
    "    print \"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\"\n",
    "    for item in sortedSF:\n",
    "        print item[0]\n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print \"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\"\n",
    "    for item in sortedNY:\n",
    "        print item[0]\n",
    "        \n",
    "getTopWords(ny, sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 예제: 문장을 긍정/부정으로 분류하기\n",
    "\n",
    "참고:http://stevenloria.com/how-to-build-a-text-classification-system-with-python-and-textblob/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beer was amazing.\n",
      "pos\n",
      "But the hangover was horrible.\n",
      "neg\n",
      "My boss was not pleased.\n",
      "neg\n",
      "Accuracy: 0.833333333333\n",
      "Most Informative Features\n",
      "          contains(this) = True              neg : pos    =      2.3 : 1.0\n",
      "          contains(this) = False             pos : neg    =      1.8 : 1.0\n",
      "          contains(This) = False             neg : pos    =      1.6 : 1.0\n",
      "            contains(an) = False             neg : pos    =      1.6 : 1.0\n",
      "             contains(I) = False             pos : neg    =      1.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob import TextBlob\n",
    "\n",
    "train = [\n",
    "    ('I love this sandwich.', 'pos'),\n",
    "    ('This is an amazing place!', 'pos'),\n",
    "    ('I feel very good about these beers.', 'pos'),\n",
    "    ('This is my best work.', 'pos'),\n",
    "    (\"What an awesome view\", 'pos'),\n",
    "    ('I do not like this restaurant', 'neg'),\n",
    "    ('I am tired of this stuff.', 'neg'),\n",
    "    (\"I can't deal with this\", 'neg'),\n",
    "    ('He is my sworn enemy!', 'neg'),\n",
    "    ('My boss is horrible.', 'neg')\n",
    "]\n",
    "test = [\n",
    "    ('The beer was good.', 'pos'),\n",
    "    ('I do not enjoy my job', 'neg'),\n",
    "    (\"I ain't feeling dandy today.\", 'neg'),\n",
    "    (\"I feel amazing!\", 'pos'),\n",
    "    ('Gary is a friend of mine.', 'pos'),\n",
    "    (\"I can't believe I'm doing this.\", 'neg')\n",
    "]\n",
    "\n",
    "cl = NaiveBayesClassifier(train)\n",
    "\n",
    "# Classify some text\n",
    "# print(cl.classify(\"Their burgers are amazing.\"))  # \"pos\"\n",
    "# print(cl.classify(\"I don't like their pizza.\"))   # \"neg\"\n",
    "\n",
    "# Classify a TextBlob\n",
    "blob = TextBlob(\"The beer was amazing. But the hangover was horrible. \"\n",
    "                \"My boss was not pleased.\", classifier=cl)\n",
    "# print(blob)\n",
    "# print(blob.classify())\n",
    "\n",
    "for sentence in blob.sentences:\n",
    "    print(sentence)\n",
    "    print(sentence.classify())\n",
    "\n",
    "# Compute accuracy\n",
    "print(\"Accuracy: {0}\".format(cl.accuracy(test)))\n",
    "\n",
    "# Show 5 most informative features\n",
    "cl.show_informative_features(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
